"""
Analytics API Views
ë„ë©”ì¸ ë° í˜ì´ì§€ë³„ SEO ì„±ê³¼ ì¶”ì  ëŒ€ì‹œë³´ë“œ API
"""
import logging
from datetime import timedelta, datetime
from collections import defaultdict

from django.db.models import Avg, Sum, Count, F, Max, Min
from django.db.models.functions import TruncDate, TruncWeek
from django.utils import timezone
from django.conf import settings
from rest_framework import viewsets, status
from rest_framework.decorators import action
from rest_framework.response import Response
from celery.schedules import crontab

from ..models import Domain, Page, SEOMetrics, SEOIssue, AIFixHistory, SEOAnalysisReport, DailyTrafficSnapshot
from ..services.search_console import SearchConsoleService

logger = logging.getLogger(__name__)


class AnalyticsViewSet(viewsets.ViewSet):
    """
    ë„ë©”ì¸ ë° í˜ì´ì§€ë³„ SEO ì„±ê³¼ ë¶„ì„ API

    Endpoints:
    - GET /analytics/domain_overview/ - ë„ë©”ì¸ ì „ì²´ ê°œìš”
    - GET /analytics/page_trends/ - í˜ì´ì§€ë³„ SEO íŠ¸ë Œë“œ
    - GET /analytics/keyword_trends/ - í‚¤ì›Œë“œ ë…¸ì¶œ íŠ¸ë Œë“œ
    - GET /analytics/comparison/ - ì‹œì‘ vs í˜„ì¬ ë¹„êµ
    """

    @action(detail=False, methods=['get'])
    def domain_overview(self, request):
        """
        ë„ë©”ì¸ ì „ì²´ ê°œìš” - ì¢…í•© ìŠ¤ì½”ì–´ ë° íŠ¸ë Œë“œ

        Query params:
        - domain_id: ë„ë©”ì¸ ID (í•„ìˆ˜)
        - days: ì¡°íšŒ ê¸°ê°„ (ê¸°ë³¸ 30ì¼)
        """
        domain_id = request.query_params.get('domain_id')
        days = int(request.query_params.get('days', 30))

        if not domain_id:
            return Response({'error': 'domain_id is required'}, status=400)

        try:
            domain = Domain.objects.get(id=domain_id)
        except Domain.DoesNotExist:
            return Response({'error': 'Domain not found'}, status=404)

        # ê¸°ê°„ ì„¤ì •
        end_date = timezone.now()
        start_date = end_date - timedelta(days=days)

        # í˜ì´ì§€ ëª©ë¡
        pages = Page.objects.filter(domain=domain).prefetch_related('seo_metrics')

        # ë„ë©”ì¸ ì „ì²´ íŠ¸ë Œë“œ (ì¼ë³„ í‰ê· )
        domain_trends = self._get_domain_trends(domain, start_date, end_date)

        # í˜„ì¬ ë„ë©”ì¸ ìƒíƒœ
        current_stats = self._get_current_domain_stats(domain)

        # ì‹œì‘ ì‹œì  vs í˜„ì¬ ë¹„êµ
        comparison = self._get_start_vs_current(domain, start_date)

        # í˜ì´ì§€ ìˆ˜
        page_count = pages.count()
        synced_pages = pages.filter(seo_metrics__isnull=False).distinct().count()

        return Response({
            'domain': {
                'id': domain.id,
                'name': domain.domain_name,
                'page_count': page_count,
                'synced_pages': synced_pages,
            },
            'current_stats': current_stats,
            'comparison': comparison,
            'trends': domain_trends,
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat(),
                'days': days,
            }
        })

    @action(detail=False, methods=['get'])
    def page_trends(self, request):
        """
        í˜ì´ì§€ë³„ SEO íŠ¸ë Œë“œ ë¦¬ìŠ¤íŠ¸

        Query params:
        - domain_id: ë„ë©”ì¸ ID (í•„ìˆ˜)
        - days: ì¡°íšŒ ê¸°ê°„ (ê¸°ë³¸ 30ì¼)
        - limit: í˜ì´ì§€ ìˆ˜ ì œí•œ (ê¸°ë³¸ 50)
        """
        domain_id = request.query_params.get('domain_id')
        days = int(request.query_params.get('days', 30))
        limit = int(request.query_params.get('limit', 50))

        if not domain_id:
            return Response({'error': 'domain_id is required'}, status=400)

        try:
            domain = Domain.objects.get(id=domain_id)
        except Domain.DoesNotExist:
            return Response({'error': 'Domain not found'}, status=404)

        end_date = timezone.now()
        start_date = end_date - timedelta(days=days)

        # ëª¨ë“  í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        pages = Page.objects.filter(domain=domain)[:limit]

        page_data = []
        for page in pages:
            trends = self._get_page_trends(page, start_date, end_date)
            comparison = self._get_page_comparison(page, start_date)

            # ì‹¤ì œ ì´ìŠˆ ê¸°ë°˜ Health Score (SEOAnalysisReportì—ì„œ)
            latest_report = page.seo_reports.order_by('-analyzed_at').first()
            actual_health_score = latest_report.overall_health_score if latest_report else None

            page_data.append({
                'page_id': page.id,
                'url': page.url,
                'path': page.path,
                'title': page.title,
                'depth_level': page.depth_level,
                'actual_health_score': actual_health_score,  # ì´ìŠˆ ê¸°ë°˜ (SEOIssuesPanelê³¼ ë™ì¼)
                'trends': trends,
                'comparison': comparison,
            })

        return Response({
            'domain_id': domain.id,
            'domain_name': domain.domain_name,
            'pages': page_data,
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat(),
                'days': days,
            }
        })

    @action(detail=False, methods=['get'])
    def keyword_trends(self, request):
        """
        í‚¤ì›Œë“œ ë…¸ì¶œ íŠ¸ë Œë“œ

        Query params:
        - domain_id: ë„ë©”ì¸ ID (í•„ìˆ˜)
        - days: ì¡°íšŒ ê¸°ê°„ (ê¸°ë³¸ 30ì¼)
        """
        domain_id = request.query_params.get('domain_id')
        days = int(request.query_params.get('days', 30))

        if not domain_id:
            return Response({'error': 'domain_id is required'}, status=400)

        try:
            domain = Domain.objects.get(id=domain_id)
        except Domain.DoesNotExist:
            return Response({'error': 'Domain not found'}, status=404)

        end_date = timezone.now()
        start_date = end_date - timedelta(days=days)

        # í˜ì´ì§€ë³„ top_queries ìˆ˜ì§‘
        pages = Page.objects.filter(domain=domain)

        keyword_data = defaultdict(lambda: {
            'impressions': 0,
            'clicks': 0,
            'pages': [],
            'first_seen': None,
            'last_seen': None,
        })

        for page in pages:
            metrics = page.seo_metrics.filter(
                snapshot_date__gte=start_date
            ).order_by('-snapshot_date')

            for metric in metrics:
                if metric.top_queries:
                    for query_data in metric.top_queries[:10]:
                        # 'query' ë˜ëŠ” 'keys[0]' í˜•ì‹ ëª¨ë‘ ì§€ì›
                        keyword = query_data.get('query') or (query_data.get('keys', [''])[0] if query_data.get('keys') else '')
                        if not keyword:
                            continue

                        kw_data = keyword_data[keyword]
                        kw_data['impressions'] += query_data.get('impressions', 0)
                        kw_data['clicks'] += query_data.get('clicks', 0)

                        if page.url not in kw_data['pages']:
                            kw_data['pages'].append(page.url)

                        snapshot_date = metric.snapshot_date
                        if not kw_data['first_seen'] or snapshot_date < kw_data['first_seen']:
                            kw_data['first_seen'] = snapshot_date
                        if not kw_data['last_seen'] or snapshot_date > kw_data['last_seen']:
                            kw_data['last_seen'] = snapshot_date

        # ìƒìœ„ í‚¤ì›Œë“œ ì •ë ¬
        sorted_keywords = sorted(
            keyword_data.items(),
            key=lambda x: x[1]['impressions'],
            reverse=True
        )[:50]

        keywords = []
        for keyword, data in sorted_keywords:
            keywords.append({
                'keyword': keyword,
                'impressions': data['impressions'],
                'clicks': data['clicks'],
                'ctr': (data['clicks'] / data['impressions'] * 100) if data['impressions'] > 0 else 0,
                'page_count': len(data['pages']),
                'pages': data['pages'][:5],  # ìƒìœ„ 5ê°œ í˜ì´ì§€ë§Œ
                'first_seen': data['first_seen'].isoformat() if data['first_seen'] else None,
                'last_seen': data['last_seen'].isoformat() if data['last_seen'] else None,
            })

        return Response({
            'domain_id': domain.id,
            'keywords': keywords,
            'total_keywords': len(keyword_data),
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat(),
                'days': days,
            }
        })

    @action(detail=False, methods=['get'])
    def comparison(self, request):
        """
        ì‹œì‘ ì‹œì  vs í˜„ì¬ ìƒì„¸ ë¹„êµ

        Query params:
        - domain_id: ë„ë©”ì¸ ID (í•„ìˆ˜)
        """
        domain_id = request.query_params.get('domain_id')

        if not domain_id:
            return Response({'error': 'domain_id is required'}, status=400)

        try:
            domain = Domain.objects.get(id=domain_id)
        except Domain.DoesNotExist:
            return Response({'error': 'Domain not found'}, status=404)

        pages = Page.objects.filter(domain=domain)

        comparison_data = []
        total_improvement = {
            'seo_score': 0,
            'impressions': 0,
            'clicks': 0,
            'keywords': 0,
            'pages_improved': 0,
            'pages_declined': 0,
        }

        for page in pages:
            metrics = page.seo_metrics.order_by('snapshot_date')

            if metrics.count() < 2:
                continue

            first = metrics.first()
            latest = metrics.last()

            # ë³€í™”ëŸ‰ ê³„ì‚°
            seo_change = (latest.seo_score or 0) - (first.seo_score or 0)
            impressions_change = (latest.impressions or 0) - (first.impressions or 0)
            clicks_change = (latest.clicks or 0) - (first.clicks or 0)

            # í‚¤ì›Œë“œ ìˆ˜ ë³€í™”
            first_keywords = len(first.top_queries) if first.top_queries else 0
            latest_keywords = len(latest.top_queries) if latest.top_queries else 0
            keywords_change = latest_keywords - first_keywords

            comparison_data.append({
                'page_id': page.id,
                'url': page.url,
                'path': page.path,
                'title': page.title,
                'first_snapshot': {
                    'date': first.snapshot_date.isoformat(),
                    'seo_score': first.seo_score,
                    'impressions': first.impressions,
                    'clicks': first.clicks,
                    'keywords_count': first_keywords,
                },
                'latest_snapshot': {
                    'date': latest.snapshot_date.isoformat(),
                    'seo_score': latest.seo_score,
                    'impressions': latest.impressions,
                    'clicks': latest.clicks,
                    'keywords_count': latest_keywords,
                },
                'changes': {
                    'seo_score': seo_change,
                    'impressions': impressions_change,
                    'clicks': clicks_change,
                    'keywords': keywords_change,
                },
                'improved': seo_change > 0,
            })

            # ì „ì²´ ì§‘ê³„
            total_improvement['seo_score'] += seo_change
            total_improvement['impressions'] += impressions_change
            total_improvement['clicks'] += clicks_change
            total_improvement['keywords'] += keywords_change

            if seo_change > 0:
                total_improvement['pages_improved'] += 1
            elif seo_change < 0:
                total_improvement['pages_declined'] += 1

        # í‰ê·  ê³„ì‚°
        page_count = len(comparison_data)
        if page_count > 0:
            total_improvement['avg_seo_change'] = total_improvement['seo_score'] / page_count
        else:
            total_improvement['avg_seo_change'] = 0

        return Response({
            'domain_id': domain.id,
            'domain_name': domain.domain_name,
            'summary': total_improvement,
            'pages': comparison_data,
        })

    # =========================================================================
    # Helper Methods
    # =========================================================================

    def _get_domain_trends(self, domain, start_date, end_date):
        """ë„ë©”ì¸ ì „ì²´ ì¼ë³„ íŠ¸ë Œë“œ (SEOMetrics + ì €ì¥ëœ íŠ¸ë˜í”½ ë°ì´í„° ë³‘í•©)"""

        # 1. SEOMetricsì—ì„œ SEO/Performance ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        # MySQL TruncDate ì´ìŠˆë¡œ Pythonì—ì„œ ë‚ ì§œ ê·¸ë£¹í•‘
        metrics_raw = SEOMetrics.objects.filter(
            page__domain=domain,
            snapshot_date__gte=start_date,
            snapshot_date__lte=end_date,
        ).values('snapshot_date', 'seo_score', 'performance_score', 'page_id')

        # Pythonì—ì„œ ë‚ ì§œë³„ ê·¸ë£¹í•‘
        metrics_by_date = {}
        for m in metrics_raw:
            if m['snapshot_date']:
                # timezone aware datetimeì„ local dateë¡œ ë³€í™˜
                local_dt = timezone.localtime(m['snapshot_date'])
                date_str = local_dt.strftime('%Y-%m-%d')

                if date_str not in metrics_by_date:
                    metrics_by_date[date_str] = {
                        'seo_scores': [],
                        'perf_scores': [],
                        'pages': set(),
                    }

                if m['seo_score'] is not None:
                    metrics_by_date[date_str]['seo_scores'].append(m['seo_score'])
                if m['performance_score'] is not None:
                    metrics_by_date[date_str]['perf_scores'].append(m['performance_score'])
                if m['page_id']:
                    metrics_by_date[date_str]['pages'].add(m['page_id'])

        # í‰ê·  ê³„ì‚°
        for date_str, data in metrics_by_date.items():
            seo_scores = data['seo_scores']
            perf_scores = data['perf_scores']
            metrics_by_date[date_str] = {
                'seo_score': round(sum(seo_scores) / len(seo_scores), 1) if seo_scores else None,
                'performance_score': round(sum(perf_scores) / len(perf_scores), 1) if perf_scores else None,
                'page_count': len(data['pages']),
            }

        # 2. ì €ì¥ëœ DailyTrafficSnapshotì—ì„œ íŠ¸ë˜í”½ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        gsc_by_date = {}
        stored_snapshots = DailyTrafficSnapshot.objects.filter(
            domain=domain,
            date__gte=start_date.date() if hasattr(start_date, 'date') else start_date,
            date__lte=end_date.date() if hasattr(end_date, 'date') else end_date,
        ).order_by('date')

        for snapshot in stored_snapshots:
            date_str = snapshot.date.strftime('%Y-%m-%d')
            gsc_by_date[date_str] = {
                'impressions': snapshot.impressions,
                'clicks': snapshot.clicks,
                'ctr': round(snapshot.ctr * 100, 2) if snapshot.ctr else 0,
                'avg_position': round(snapshot.avg_position, 1) if snapshot.avg_position else 0,
            }

        # 3. ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•˜ë©´ GSC APIë¡œ ê°€ì ¸ì˜¤ê¸°
        if len(gsc_by_date) < 7 and domain.search_console_connected:
            try:
                gsc = SearchConsoleService()
                site_url = f'sc-domain:{domain.domain_name}'

                # GSC APIì— date ì°¨ì›ìœ¼ë¡œ ì¿¼ë¦¬
                gsc_result = gsc.get_search_analytics(
                    site_url=site_url,
                    start_date=start_date.strftime('%Y-%m-%d'),
                    end_date=end_date.strftime('%Y-%m-%d'),
                    dimensions=['date'],
                    row_limit=500
                )

                if not gsc_result.get('error'):
                    for row in gsc_result.get('rows', []):
                        date_str = row.get('keys', [''])[0]
                        if date_str and date_str not in gsc_by_date:
                            gsc_by_date[date_str] = {
                                'impressions': row.get('impressions', 0),
                                'clicks': row.get('clicks', 0),
                                'ctr': round(row.get('ctr', 0) * 100, 2),
                                'avg_position': round(row.get('position', 0), 1),
                            }
                    logger.info(f"GSC daily trends fetched from API: {len(gsc_by_date)} days for {domain.domain_name}")
            except Exception as e:
                logger.warning(f"Failed to fetch GSC daily trends: {e}")

        # 3. ëª¨ë“  ë‚ ì§œ ìˆ˜ì§‘ (SEOMetrics + GSC)
        all_dates = set(metrics_by_date.keys()) | set(gsc_by_date.keys())

        # 4. ë³‘í•©í•˜ì—¬ íŠ¸ë Œë“œ ìƒì„±
        trends = []
        for date_str in sorted(all_dates):
            seo_data = metrics_by_date.get(date_str, {})
            gsc_data = gsc_by_date.get(date_str, {})

            seo_score = seo_data.get('seo_score')
            performance_score = seo_data.get('performance_score')

            # Health Score ê³„ì‚° (SEO + Performance í‰ê· )
            health_score = None
            if seo_score is not None and performance_score is not None:
                health_score = (seo_score + performance_score) / 2
            elif seo_score is not None:
                health_score = seo_score

            trends.append({
                'date': date_str,
                'seo_score': seo_score,
                'health_score': round(health_score, 1) if health_score else None,
                'performance_score': performance_score,
                'impressions': gsc_data.get('impressions', 0),
                'clicks': gsc_data.get('clicks', 0),
                'ctr': gsc_data.get('ctr'),
                'avg_position': gsc_data.get('avg_position'),
                'page_count': seo_data.get('page_count', 0),
            })

        return trends

    def _get_current_domain_stats(self, domain):
        """í˜„ì¬ ë„ë©”ì¸ í†µê³„"""
        # ê° í˜ì´ì§€ì˜ ìµœì‹  ë©”íŠ¸ë¦­
        pages = Page.objects.filter(domain=domain)

        total_lighthouse_seo = 0
        total_health = 0
        total_performance = 0
        total_impressions = 0
        total_clicks = 0
        total_keywords = set()
        indexed_count = 0
        page_count = 0
        health_count = 0  # ì‹¤ì œ health scoreê°€ ìˆëŠ” í˜ì´ì§€ ìˆ˜

        for page in pages:
            latest = page.seo_metrics.order_by('-snapshot_date').first()
            if latest:
                page_count += 1
                total_lighthouse_seo += latest.seo_score or 0
                total_performance += latest.performance_score or 0
                total_impressions += latest.impressions or 0
                total_clicks += latest.clicks or 0

                if latest.is_indexed:
                    indexed_count += 1

                if latest.top_queries:
                    for q in latest.top_queries:
                        # 'query' ë˜ëŠ” 'keys[0]' í˜•ì‹ ëª¨ë‘ ì§€ì›
                        keyword = q.get('query') or (q.get('keys', [''])[0] if q.get('keys') else '')
                        if keyword:
                            total_keywords.add(keyword)

            # ì‹¤ì œ ì´ìŠˆ ê¸°ë°˜ Health Score (SEOAnalysisReportì—ì„œ)
            latest_report = page.seo_reports.order_by('-analyzed_at').first()
            if latest_report and latest_report.overall_health_score:
                total_health += latest_report.overall_health_score
                health_count += 1

        avg_lighthouse_seo = total_lighthouse_seo / page_count if page_count > 0 else 0
        avg_performance = total_performance / page_count if page_count > 0 else 0
        avg_health = total_health / health_count if health_count > 0 else 0
        ctr = (total_clicks / total_impressions * 100) if total_impressions > 0 else 0

        # ë„ë©”ì¸ ëŒ€í‘œ ìŠ¤ì½”ì–´ (ê°€ì¤‘ í‰ê· )
        # Health Score(ì´ìŠˆ ê¸°ë°˜) 50% + Performance 25% + ì¸ë±ì‹±ë¥  15% + CTR 10%
        indexing_rate = (indexed_count / page_count * 100) if page_count > 0 else 0
        domain_score = (
            avg_health * 0.5 +
            avg_performance * 0.25 +
            indexing_rate * 0.15 +
            min(ctr * 10, 100) * 0.10  # CTRì€ 10% = 100ì ìœ¼ë¡œ ìŠ¤ì¼€ì¼
        )

        return {
            'lighthouse_seo_score': round(avg_lighthouse_seo, 1),  # Lighthouse ê¸°ìˆ  ì ìˆ˜
            'health_score': round(avg_health, 1),  # ì´ìŠˆ ê¸°ë°˜ ì ìˆ˜ (SEOIssuesPanelê³¼ ë™ì¼)
            'performance_score': round(avg_performance, 1),
            'domain_score': round(domain_score, 1),
            'impressions': total_impressions,
            'clicks': total_clicks,
            'ctr': round(ctr, 2),
            'keyword_count': len(total_keywords),
            'indexed_pages': indexed_count,
            'total_pages': page_count,
            'indexing_rate': round(indexing_rate, 1),
        }

    def _get_start_vs_current(self, domain, start_date):
        """ì‹œì‘ ì‹œì  vs í˜„ì¬ ë¹„êµ"""
        pages = Page.objects.filter(domain=domain)

        start_stats = {'seo': 0, 'impressions': 0, 'clicks': 0, 'count': 0}
        current_stats = {'seo': 0, 'impressions': 0, 'clicks': 0, 'count': 0}
        # unique í‚¤ì›Œë“œ ìˆ˜ì§‘ì„ ìœ„í•œ set
        start_keywords = set()
        current_keywords = set()

        for page in pages:
            # ì‹œì‘ ì‹œì  (start_date ì´í›„ ì²« ë©”íŠ¸ë¦­)
            first = page.seo_metrics.filter(
                snapshot_date__gte=start_date
            ).order_by('snapshot_date').first()

            # í˜„ì¬ (ìµœì‹  ë©”íŠ¸ë¦­)
            latest = page.seo_metrics.order_by('-snapshot_date').first()

            if first:
                start_stats['seo'] += first.seo_score or 0
                start_stats['impressions'] += first.impressions or 0
                start_stats['clicks'] += first.clicks or 0
                start_stats['count'] += 1
                # unique í‚¤ì›Œë“œ ìˆ˜ì§‘
                if first.top_queries:
                    for q in first.top_queries:
                        kw = q.get('query') or (q.get('keys', [''])[0] if q.get('keys') else '')
                        if kw:
                            start_keywords.add(kw)

            if latest:
                current_stats['seo'] += latest.seo_score or 0
                current_stats['impressions'] += latest.impressions or 0
                current_stats['clicks'] += latest.clicks or 0
                current_stats['count'] += 1
                # unique í‚¤ì›Œë“œ ìˆ˜ì§‘
                if latest.top_queries:
                    for q in latest.top_queries:
                        kw = q.get('query') or (q.get('keys', [''])[0] if q.get('keys') else '')
                        if kw:
                            current_keywords.add(kw)

        # í‰ê·  ê³„ì‚°
        start_avg_seo = start_stats['seo'] / start_stats['count'] if start_stats['count'] > 0 else 0
        current_avg_seo = current_stats['seo'] / current_stats['count'] if current_stats['count'] > 0 else 0

        return {
            'start': {
                'date': start_date.isoformat(),
                'avg_seo_score': round(start_avg_seo, 1),
                'total_impressions': start_stats['impressions'],
                'total_clicks': start_stats['clicks'],
                'total_keywords': len(start_keywords),
                'page_count': start_stats['count'],
            },
            'current': {
                'avg_seo_score': round(current_avg_seo, 1),
                'total_impressions': current_stats['impressions'],
                'total_clicks': current_stats['clicks'],
                'total_keywords': len(current_keywords),
                'page_count': current_stats['count'],
            },
            'changes': {
                'seo_score': round(current_avg_seo - start_avg_seo, 1),
                'impressions': current_stats['impressions'] - start_stats['impressions'],
                'clicks': current_stats['clicks'] - start_stats['clicks'],
                'keywords': len(current_keywords) - len(start_keywords),
                'seo_percent': round((current_avg_seo - start_avg_seo) / start_avg_seo * 100, 1) if start_avg_seo > 0 else 0,
            }
        }

    def _get_page_trends(self, page, start_date, end_date):
        """í˜ì´ì§€ë³„ íŠ¸ë Œë“œ ë°ì´í„°"""
        metrics = page.seo_metrics.filter(
            snapshot_date__gte=start_date,
            snapshot_date__lte=end_date,
        ).order_by('snapshot_date')

        trends = []
        for m in metrics:
            # Health Score
            health_score = None
            if m.seo_score is not None and m.performance_score is not None:
                health_score = (m.seo_score + m.performance_score) / 2
            elif m.seo_score is not None:
                health_score = m.seo_score

            # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ
            # GSC APIëŠ” í‚¤ì›Œë“œë¥¼ 'keys' ë°°ì—´ì— ì €ì¥í•¨ (ì˜ˆ: {"keys": ["ì½”ì¸ê·¸ë¦¬"], ...})
            top_keywords = []
            if m.top_queries:
                for q in m.top_queries[:5]:
                    # 'query' ë˜ëŠ” 'keys[0]' í˜•ì‹ ëª¨ë‘ ì§€ì›
                    query_text = q.get('query') or (q.get('keys', [''])[0] if q.get('keys') else '')
                    if query_text:  # ë¹ˆ ë¬¸ìì—´ ì œì™¸
                        top_keywords.append({
                            'query': query_text,
                            'impressions': q.get('impressions', 0),
                            'clicks': q.get('clicks', 0),
                            'position': q.get('position', 0),
                        })

            trends.append({
                'date': m.snapshot_date.isoformat(),
                'seo_score': m.seo_score,
                'health_score': round(health_score, 1) if health_score else None,
                'performance_score': m.performance_score,
                'impressions': m.impressions or 0,
                'clicks': m.clicks or 0,
                'ctr': round(m.ctr, 2) if m.ctr else None,
                'keywords_count': len(m.top_queries) if m.top_queries else 0,
                'top_keywords': top_keywords,  # í‚¤ì›Œë“œ ëª©ë¡ ì¶”ê°€
            })

        return trends

    def _get_page_comparison(self, page, start_date):
        """í˜ì´ì§€ ì‹œì‘ vs í˜„ì¬ ë¹„êµ"""
        first = page.seo_metrics.filter(
            snapshot_date__gte=start_date
        ).order_by('snapshot_date').first()

        latest = page.seo_metrics.order_by('-snapshot_date').first()

        if not first or not latest:
            return None

        return {
            'start': {
                'date': first.snapshot_date.isoformat(),
                'seo_score': first.seo_score,
                'impressions': first.impressions or 0,
                'clicks': first.clicks or 0,
            },
            'current': {
                'date': latest.snapshot_date.isoformat(),
                'seo_score': latest.seo_score,
                'impressions': latest.impressions or 0,
                'clicks': latest.clicks or 0,
            },
            'changes': {
                'seo_score': (latest.seo_score or 0) - (first.seo_score or 0),
                'impressions': (latest.impressions or 0) - (first.impressions or 0),
                'clicks': (latest.clicks or 0) - (first.clicks or 0),
            }
        }

    # =========================================================================
    # Schedule Status & Settings
    # =========================================================================

    @action(detail=False, methods=['get'])
    def schedule_status(self, request):
        """
        ìŠ¤ì¼€ì¤„ ìƒíƒœ ì¡°íšŒ - í˜„ì¬ ìë™ ë™ê¸°í™” ìŠ¤ì¼€ì¤„ ë° ë§ˆì§€ë§‰ ì‹¤í–‰ ì •ë³´

        Query params:
        - domain_id: ë„ë©”ì¸ ID (í•„ìˆ˜)
        """
        domain_id = request.query_params.get('domain_id')

        if not domain_id:
            return Response({'error': 'domain_id is required'}, status=400)

        try:
            domain = Domain.objects.get(id=domain_id)
        except Domain.DoesNotExist:
            return Response({'error': 'Domain not found'}, status=404)

        # Celery Beat ìŠ¤ì¼€ì¤„ ì •ë³´
        beat_schedule = getattr(settings, 'CELERY_BEAT_SCHEDULE', {})

        # ìŠ¤ì¼€ì¤„ ì •ë³´ íŒŒì‹±
        schedules = []

        schedule_info = {
            'gsc-sync-morning': {
                'name': 'GSC ì•„ì¹¨ ë™ê¸°í™”',
                'description': 'Google Search Console ë°ì´í„° ë™ê¸°í™”',
                'icon': 'ğŸŒ…',
                'type': 'gsc',
            },
            'gsc-sync-evening': {
                'name': 'GSC ì €ë… ë™ê¸°í™”',
                'description': 'Google Search Console ë°ì´í„° ë™ê¸°í™”',
                'icon': 'ğŸŒ†',
                'type': 'gsc',
            },
            'daily-full-scan': {
                'name': 'ì¼ì¼ ì „ì²´ ìŠ¤ìº”',
                'description': 'PageSpeed APIë¥¼ ì‚¬ìš©í•œ ì „ì²´ SEO ë¶„ì„',
                'icon': 'ğŸ“Š',
                'type': 'full_scan',
            },
            'daily-ai-analysis': {
                'name': 'AI ì¼ì¼ ë¶„ì„',
                'description': 'AI ê¸°ë°˜ SEO ê°œì„  ì œì•ˆ ìƒì„±',
                'icon': 'ğŸ§ ',
                'type': 'ai_analysis',
            },
            'vector-embedding-update': {
                'name': 'ë²¡í„° ì„ë² ë”© ì—…ë°ì´íŠ¸',
                'description': 'AI í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ë²¡í„°í™”',
                'icon': 'ğŸ”„',
                'type': 'embedding',
            },
            'evaluate-fix-effectiveness': {
                'name': 'ìˆ˜ì • íš¨ê³¼ì„± í‰ê°€',
                'description': 'ì ìš©ëœ AI ìˆ˜ì •ì˜ íš¨ê³¼ ë¶„ì„',
                'icon': 'ğŸ“ˆ',
                'type': 'evaluation',
            },
            'daily-snapshot': {
                'name': 'ì¼ì¼ ìŠ¤ëƒ…ìƒ·',
                'description': 'SEO ë©”íŠ¸ë¦­ ì¼ì¼ ìŠ¤ëƒ…ìƒ· ìƒì„±',
                'icon': 'ğŸ“¸',
                'type': 'snapshot',
            },
        }

        # DBì—ì„œ ì‚¬ìš©ì ì •ì˜ ìŠ¤ì¼€ì¤„ ì¡°íšŒ (ì˜¤ë²„ë¼ì´ë“œ)
        from django_celery_beat.models import PeriodicTask
        db_overrides = {task.name: task for task in PeriodicTask.objects.all()}

        for key, schedule_config in beat_schedule.items():
            info = schedule_info.get(key, {
                'name': key,
                'description': '',
                'icon': 'â°',
                'type': 'other',
            })

            # DBì— ì˜¤ë²„ë¼ì´ë“œê°€ ìˆìœ¼ë©´ DB ê°’ ì‚¬ìš©
            if key in db_overrides:
                db_task = db_overrides[key]
                cron = db_task.crontab
                if cron:
                    hour = int(cron.hour) if cron.hour.isdigit() else cron.hour
                    minute = int(cron.minute) if cron.minute.isdigit() else 0
                    if isinstance(hour, int):
                        schedule_text = f'ë§¤ì¼ {hour:02d}:{minute:02d}'
                        # DB ìŠ¤ì¼€ì¤„ë¡œ ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
                        next_run = self._calculate_next_run_from_crontab(hour, minute)
                    else:
                        schedule_text = f'{hour}ì‹œê°„ë§ˆë‹¤'
                        next_run = None
                else:
                    schedule_text = 'Unknown'
                    next_run = None
                enabled = db_task.enabled
            else:
                # settings.pyì—ì„œ ê°€ì ¸ì˜´
                schedule = schedule_config.get('schedule')
                schedule_text = self._format_crontab(schedule) if schedule else 'Unknown'
                next_run = self._calculate_next_run(schedule) if schedule else None
                enabled = True

            schedules.append({
                'key': key,
                'name': info['name'],
                'description': info['description'],
                'icon': info['icon'],
                'type': info['type'],
                'task': schedule_config.get('task', ''),
                'schedule_text': schedule_text,
                'next_run': next_run.isoformat() if next_run else None,
                'enabled': enabled,
                'editable': key in ['daily-full-scan', 'daily-ai-analysis', 'gsc-sync-morning', 'gsc-sync-evening', 'daily-snapshot', 'evaluate-fix-effectiveness'],
            })

        # ë„ë©”ì¸ë³„ ë§ˆì§€ë§‰ ë™ê¸°í™” ì •ë³´ (ìƒˆ í•„ë“œ ì‚¬ìš©)
        last_gsc_sync = domain.last_gsc_sync_at
        last_full_scan = domain.last_full_scan_at or domain.last_scanned_at
        gsc_sync_status = domain.gsc_sync_status or 'idle'
        full_scan_status = domain.full_scan_status or 'idle'

        # AILearningStateì—ì„œ ë§ˆì§€ë§‰ ë™ê¸°í™” ì •ë³´
        try:
            ai_state = domain.ai_learning_state
            last_ai_sync = ai_state.last_sync_at
            ai_sync_status = ai_state.sync_status
        except Exception:
            last_ai_sync = None
            ai_sync_status = 'idle'

        # ìƒˆ í•„ë“œê°€ ì—†ìœ¼ë©´ SEOMetricsì—ì„œ ì¶”ì • (í•˜ìœ„ í˜¸í™˜ì„±)
        if not last_gsc_sync:
            latest_metric = SEOMetrics.objects.filter(
                page__domain=domain
            ).order_by('-snapshot_date').first()
            if latest_metric:
                last_gsc_sync = latest_metric.snapshot_date

        return Response({
            'domain': {
                'id': domain.id,
                'name': domain.domain_name,
            },
            'schedules': schedules,
            'last_sync': {
                'gsc': last_gsc_sync.isoformat() if last_gsc_sync else None,
                'full_scan': last_full_scan.isoformat() if last_full_scan else None,
                'ai_sync': last_ai_sync.isoformat() if last_ai_sync else None,
            },
            'sync_status': {
                'gsc': gsc_sync_status,
                'full_scan': full_scan_status,
                'ai': ai_sync_status,
                'domain': domain.status,
            },
            'gsc_connected': domain.search_console_connected,
        })

    @action(detail=False, methods=['post'])
    def trigger_sync(self, request):
        """
        ìˆ˜ë™ ë™ê¸°í™” íŠ¸ë¦¬ê±°

        Request body:
        - domain_id: ë„ë©”ì¸ ID (í•„ìˆ˜)
        - sync_type: ë™ê¸°í™” ìœ í˜• ('gsc', 'full_scan', 'ai_analysis')
        """
        domain_id = request.data.get('domain_id')
        sync_type = request.data.get('sync_type', 'gsc')

        if not domain_id:
            return Response({'error': 'domain_id is required'}, status=400)

        try:
            domain = Domain.objects.get(id=domain_id)
        except Domain.DoesNotExist:
            return Response({'error': 'Domain not found'}, status=404)

        # Celery íƒœìŠ¤í¬ ì„í¬íŠ¸ ë° ì‹¤í–‰
        from ..tasks import gsc_sync_domain, refresh_domain_cache, ai_auto_analysis

        task_result = None
        task_name = ''

        if sync_type == 'gsc':
            task_result = gsc_sync_domain.delay(domain.id)
            task_name = 'GSC ë™ê¸°í™”'
        elif sync_type == 'full_scan':
            task_result = refresh_domain_cache.delay(domain.id)
            task_name = 'ì „ì²´ ìŠ¤ìº”'
        elif sync_type == 'ai_analysis':
            task_result = ai_auto_analysis.delay(domain.id)
            task_name = 'AI ë¶„ì„'
        else:
            return Response({'error': f'Unknown sync_type: {sync_type}'}, status=400)

        return Response({
            'success': True,
            'message': f'{task_name} ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.',
            'task_id': task_result.id if task_result else None,
            'sync_type': sync_type,
            'domain_id': domain.id,
        })

    def _format_crontab(self, schedule):
        """crontab ìŠ¤ì¼€ì¤„ì„ ì½ê¸° ì‰¬ìš´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not isinstance(schedule, crontab):
            return str(schedule)

        hour = schedule._orig_hour
        minute = schedule._orig_minute
        dow = schedule._orig_day_of_week

        # ì‹œê°„ í¬ë§·
        if isinstance(hour, str) and '/' in hour:
            # */6 í˜•ì‹
            interval = hour.split('/')[1]
            return f'{interval}ì‹œê°„ë§ˆë‹¤'

        # ìš”ì¼ í™•ì¸
        if dow != '*':
            day_names = ['ì¼', 'ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ']
            if isinstance(dow, int):
                dow_text = day_names[dow]
            else:
                dow_text = str(dow)
            return f'ë§¤ì£¼ {dow_text}ìš”ì¼ {hour}:{minute:02d}'

        # ë§¤ì¼
        if hour != '*':
            return f'ë§¤ì¼ {hour}:{minute:02d}'

        return f'{minute}ë¶„ë§ˆë‹¤'

    def _calculate_next_run(self, schedule):
        """ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚° (KST ê¸°ì¤€)"""
        if not isinstance(schedule, crontab):
            return None

        # KSTë¡œ ë³€í™˜í•˜ì—¬ ê³„ì‚° (ìŠ¤ì¼€ì¤„ì´ KST ê¸°ì¤€ì´ë¯€ë¡œ)
        now = timezone.localtime(timezone.now())

        hour = schedule._orig_hour
        minute = schedule._orig_minute
        dow = schedule._orig_day_of_week

        # ê°„ë‹¨í•œ ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        if isinstance(hour, str) and '/' in hour:
            # */N ì‹œê°„ë§ˆë‹¤
            interval = int(hour.split('/')[1])
            next_hour = ((now.hour // interval) + 1) * interval
            if next_hour >= 24:
                next_run = now.replace(hour=0, minute=0, second=0) + timedelta(days=1)
            else:
                next_run = now.replace(hour=next_hour, minute=0, second=0)
            return next_run

        if hour != '*':
            hour = int(hour)
            minute = int(minute) if minute != '*' else 0

            next_run = now.replace(hour=hour, minute=minute, second=0, microsecond=0)

            if next_run <= now:
                next_run += timedelta(days=1)

            # ìš”ì¼ í™•ì¸
            if dow != '*':
                target_dow = int(dow)
                current_dow = next_run.weekday()
                # Python: 0=ì›”, 6=ì¼, Celery: 0=ì¼, 6=í† 
                celery_dow = (current_dow + 1) % 7
                days_until = (target_dow - celery_dow) % 7
                if days_until == 0 and next_run <= now:
                    days_until = 7
                next_run += timedelta(days=days_until)

            return next_run

        return None

    def _calculate_next_run_from_crontab(self, hour, minute):
        """DB crontabì—ì„œ ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°"""
        now = timezone.localtime(timezone.now())

        next_run = now.replace(hour=hour, minute=minute, second=0, microsecond=0)

        if next_run <= now:
            next_run += timedelta(days=1)

        return next_run

    @action(detail=False, methods=['post'])
    def update_schedule(self, request):
        """
        ìŠ¤ì¼€ì¤„ ì‹œê°„ ì—…ë°ì´íŠ¸

        Request body:
        - schedule_key: ìŠ¤ì¼€ì¤„ í‚¤ (ì˜ˆ: 'daily-full-scan')
        - hour: ì‹¤í–‰ ì‹œê°„ (0-23)
        - minute: ì‹¤í–‰ ë¶„ (0-59), ê¸°ë³¸ê°’ 0
        - enabled: í™œì„±í™” ì—¬ë¶€, ê¸°ë³¸ê°’ True
        """
        from django_celery_beat.models import PeriodicTask, CrontabSchedule
        import json

        schedule_key = request.data.get('schedule_key')
        hour = request.data.get('hour')
        minute = request.data.get('minute', 0)
        enabled = request.data.get('enabled', True)

        if not schedule_key:
            return Response({'error': 'schedule_key is required'}, status=400)

        if hour is None:
            return Response({'error': 'hour is required'}, status=400)

        # ìœ íš¨í•œ ìŠ¤ì¼€ì¤„ í‚¤ í™•ì¸
        valid_keys = {
            'gsc-sync-morning': 'seo_analyzer.tasks.gsc_sync_all_domains',
            'gsc-sync-evening': 'seo_analyzer.tasks.gsc_sync_all_domains',
            'daily-full-scan': 'seo_analyzer.tasks.nightly_cache_update',
            'daily-ai-analysis': 'seo_analyzer.tasks.schedule_all_domain_analysis',
            'vector-embedding-update': 'seo_analyzer.tasks.update_vector_embeddings',
            'evaluate-fix-effectiveness': 'seo_analyzer.tasks.evaluate_fix_effectiveness',
            'daily-snapshot': 'seo_analyzer.tasks.generate_daily_snapshot',
        }

        if schedule_key not in valid_keys:
            return Response({
                'error': f'Invalid schedule_key: {schedule_key}',
                'valid_keys': list(valid_keys.keys())
            }, status=400)

        try:
            hour = int(hour)
            minute = int(minute)

            if not (0 <= hour <= 23):
                return Response({'error': 'hour must be between 0 and 23'}, status=400)
            if not (0 <= minute <= 59):
                return Response({'error': 'minute must be between 0 and 59'}, status=400)

            # Crontab ìŠ¤ì¼€ì¤„ ìƒì„± ë˜ëŠ” ì¡°íšŒ
            crontab_schedule, _ = CrontabSchedule.objects.get_or_create(
                minute=str(minute),
                hour=str(hour),
                day_of_week='*',
                day_of_month='*',
                month_of_year='*',
                timezone='Asia/Seoul'
            )

            # PeriodicTask ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
            task_name = valid_keys[schedule_key]

            periodic_task, created = PeriodicTask.objects.update_or_create(
                name=schedule_key,
                defaults={
                    'task': task_name,
                    'crontab': crontab_schedule,
                    'enabled': enabled,
                    'kwargs': json.dumps({}),
                }
            )

            return Response({
                'success': True,
                'message': f'ìŠ¤ì¼€ì¤„ì´ {"ìƒì„±" if created else "ì—…ë°ì´íŠ¸"}ë˜ì—ˆìŠµë‹ˆë‹¤.',
                'schedule': {
                    'key': schedule_key,
                    'hour': hour,
                    'minute': minute,
                    'enabled': enabled,
                    'schedule_text': f'ë§¤ì¼ {hour:02d}:{minute:02d}',
                }
            })

        except ValueError as e:
            return Response({'error': f'Invalid value: {str(e)}'}, status=400)
        except Exception as e:
            logger.exception(f'Failed to update schedule: {e}')
            return Response({'error': str(e)}, status=500)

    @action(detail=False, methods=['get'])
    def get_schedule_config(self, request):
        """
        ìŠ¤ì¼€ì¤„ ì„¤ì • ì¡°íšŒ (DBì—ì„œ ì‚¬ìš©ì ì •ì˜ ìŠ¤ì¼€ì¤„ í¬í•¨)

        Query params:
        - schedule_key: íŠ¹ì • ìŠ¤ì¼€ì¤„ë§Œ ì¡°íšŒ (ì„ íƒ)
        """
        from django_celery_beat.models import PeriodicTask

        schedule_key = request.query_params.get('schedule_key')

        # settings.pyì˜ ê¸°ë³¸ ìŠ¤ì¼€ì¤„
        beat_schedule = getattr(settings, 'CELERY_BEAT_SCHEDULE', {})

        # DBì—ì„œ ì‚¬ìš©ì ì •ì˜ ìŠ¤ì¼€ì¤„ ì¡°íšŒ
        db_schedules = PeriodicTask.objects.filter(enabled=True)

        schedule_configs = []

        schedule_info = {
            'gsc-sync-morning': {'name': 'GSC ì•„ì¹¨ ë™ê¸°í™”', 'type': 'gsc', 'editable': True},
            'gsc-sync-evening': {'name': 'GSC ì €ë… ë™ê¸°í™”', 'type': 'gsc', 'editable': True},
            'daily-full-scan': {'name': 'ì¼ì¼ ì „ì²´ ìŠ¤ìº”', 'type': 'full_scan', 'editable': True},
            'daily-ai-analysis': {'name': 'AI ì¼ì¼ ë¶„ì„', 'type': 'ai_analysis', 'editable': True},
            'vector-embedding-update': {'name': 'ë²¡í„° ì„ë² ë”© ì—…ë°ì´íŠ¸', 'type': 'embedding', 'editable': False},
            'evaluate-fix-effectiveness': {'name': 'ìˆ˜ì • íš¨ê³¼ì„± í‰ê°€', 'type': 'evaluation', 'editable': True},
            'daily-snapshot': {'name': 'ì¼ì¼ ìŠ¤ëƒ…ìƒ·', 'type': 'snapshot', 'editable': True},
        }

        # DB ìŠ¤ì¼€ì¤„ë¡œ ì˜¤ë²„ë¼ì´ë“œëœ í‚¤ ì¶”ì 
        db_overrides = {task.name: task for task in db_schedules}

        for key, config in beat_schedule.items():
            if schedule_key and key != schedule_key:
                continue

            info = schedule_info.get(key, {'name': key, 'type': 'other', 'editable': False})

            # DBì— ì˜¤ë²„ë¼ì´ë“œê°€ ìˆìœ¼ë©´ DB ê°’ ì‚¬ìš©
            if key in db_overrides:
                db_task = db_overrides[key]
                cron = db_task.crontab
                if cron:
                    hour = int(cron.hour) if cron.hour.isdigit() else cron.hour
                    minute = int(cron.minute) if cron.minute.isdigit() else cron.minute
                    schedule_text = f'ë§¤ì¼ {hour:02d}:{minute:02d}' if isinstance(hour, int) else cron.hour
                    enabled = db_task.enabled
                else:
                    hour = None
                    minute = None
                    schedule_text = 'Unknown'
                    enabled = db_task.enabled
                source = 'database'
            else:
                # settings.pyì—ì„œ ê°€ì ¸ì˜´
                schedule = config.get('schedule')
                schedule_text = self._format_crontab(schedule) if schedule else 'Unknown'
                hour = schedule._orig_hour if isinstance(schedule, crontab) else None
                minute = schedule._orig_minute if isinstance(schedule, crontab) else None
                enabled = True
                source = 'settings'

            schedule_configs.append({
                'key': key,
                'name': info['name'],
                'type': info['type'],
                'editable': info['editable'],
                'hour': int(hour) if isinstance(hour, (int, str)) and str(hour).isdigit() else hour,
                'minute': int(minute) if isinstance(minute, (int, str)) and str(minute).isdigit() else minute,
                'schedule_text': schedule_text,
                'enabled': enabled,
                'source': source,
            })

        return Response({
            'schedules': schedule_configs
        })

    @action(detail=False, methods=['post'])
    def backfill_gsc_traffic(self, request):
        """
        GSC ê³¼ê±° íŠ¸ë˜í”½ ë°ì´í„°ë¥¼ DBì— ì €ì¥ (ìµœì´ˆ 1íšŒ ë˜ëŠ” ìˆ˜ë™ ì‹¤í–‰)

        Request body:
        - domain_id: ë„ë©”ì¸ ID (í•„ìˆ˜)
        - days: ê°€ì ¸ì˜¬ ê¸°ê°„ (ê¸°ë³¸ 90ì¼, ìµœëŒ€ 500ì¼)
        """
        domain_id = request.data.get('domain_id')
        days = int(request.data.get('days', 90))

        if not domain_id:
            return Response({'error': 'domain_id is required'}, status=400)

        if days > 500:
            days = 500  # GSC API ì œí•œ

        try:
            domain = Domain.objects.get(id=domain_id)
        except Domain.DoesNotExist:
            return Response({'error': 'Domain not found'}, status=404)

        if not domain.search_console_connected:
            return Response({
                'error': 'GSC not connected',
                'message': 'Google Search Consoleì´ ì—°ê²°ë˜ì§€ ì•Šì€ ë„ë©”ì¸ì…ë‹ˆë‹¤.'
            }, status=400)

        # ê¸°ê°„ ì„¤ì •
        end_date = timezone.now()
        start_date = end_date - timedelta(days=days)

        try:
            gsc = SearchConsoleService()
            site_url = f'sc-domain:{domain.domain_name}'

            # GSC APIì—ì„œ ì¼ë³„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            gsc_result = gsc.get_search_analytics(
                site_url=site_url,
                start_date=start_date.strftime('%Y-%m-%d'),
                end_date=end_date.strftime('%Y-%m-%d'),
                dimensions=['date'],
                row_limit=500
            )

            if gsc_result.get('error'):
                return Response({
                    'error': 'GSC API error',
                    'message': gsc_result.get('error')
                }, status=500)

            rows = gsc_result.get('rows', [])
            created_count = 0
            updated_count = 0

            for row in rows:
                date_str = row.get('keys', [''])[0]
                if not date_str:
                    continue

                # date_str: 'YYYY-MM-DD'
                try:
                    snapshot_date = datetime.strptime(date_str, '%Y-%m-%d').date()
                except ValueError:
                    continue

                # DailyTrafficSnapshot ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
                snapshot, created = DailyTrafficSnapshot.objects.update_or_create(
                    domain=domain,
                    date=snapshot_date,
                    defaults={
                        'impressions': row.get('impressions', 0),
                        'clicks': row.get('clicks', 0),
                        'ctr': row.get('ctr', 0),
                        'avg_position': row.get('position', 0),
                    }
                )

                if created:
                    created_count += 1
                else:
                    updated_count += 1

            logger.info(f"Backfill completed for {domain.domain_name}: {created_count} created, {updated_count} updated")

            return Response({
                'success': True,
                'message': f'GSC íŠ¸ë˜í”½ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.',
                'domain_id': domain.id,
                'domain_name': domain.domain_name,
                'period': {
                    'start': start_date.strftime('%Y-%m-%d'),
                    'end': end_date.strftime('%Y-%m-%d'),
                    'days': days,
                },
                'stats': {
                    'fetched_rows': len(rows),
                    'created': created_count,
                    'updated': updated_count,
                }
            })

        except Exception as e:
            logger.exception(f'Failed to backfill GSC traffic: {e}')
            return Response({
                'error': str(e),
                'message': 'GSC ë°ì´í„° ê°€ì ¸ì˜¤ê¸°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'
            }, status=500)

    @action(detail=False, methods=['get'])
    def traffic_history(self, request):
        """
        ì €ì¥ëœ íŠ¸ë˜í”½ íˆìŠ¤í† ë¦¬ ì¡°íšŒ

        Query params:
        - domain_id: ë„ë©”ì¸ ID (í•„ìˆ˜)
        - days: ì¡°íšŒ ê¸°ê°„ (ê¸°ë³¸ 30ì¼)
        """
        domain_id = request.query_params.get('domain_id')
        days = int(request.query_params.get('days', 30))

        if not domain_id:
            return Response({'error': 'domain_id is required'}, status=400)

        try:
            domain = Domain.objects.get(id=domain_id)
        except Domain.DoesNotExist:
            return Response({'error': 'Domain not found'}, status=404)

        end_date = timezone.now().date()
        start_date = end_date - timedelta(days=days)

        # ì €ì¥ëœ ìŠ¤ëƒ…ìƒ· ì¡°íšŒ
        snapshots = DailyTrafficSnapshot.objects.filter(
            domain=domain,
            date__gte=start_date,
            date__lte=end_date
        ).order_by('date')

        # ì´ê³„ ê³„ì‚°
        total_impressions = sum(s.impressions for s in snapshots)
        total_clicks = sum(s.clicks for s in snapshots)
        avg_ctr = total_clicks / total_impressions * 100 if total_impressions > 0 else 0

        data = []
        for s in snapshots:
            data.append({
                'date': s.date.strftime('%Y-%m-%d'),
                'impressions': s.impressions,
                'clicks': s.clicks,
                'ctr': round(s.ctr * 100, 2) if s.ctr else 0,
                'avg_position': round(s.avg_position, 1) if s.avg_position else 0,
            })

        return Response({
            'domain_id': domain.id,
            'domain_name': domain.domain_name,
            'period': {
                'start': start_date.strftime('%Y-%m-%d'),
                'end': end_date.strftime('%Y-%m-%d'),
                'days': days,
            },
            'stats': {
                'total_impressions': total_impressions,
                'total_clicks': total_clicks,
                'avg_ctr': round(avg_ctr, 2),
                'data_points': len(data),
            },
            'history': data,
        })
