"""
AI SEO Advisor Service
í˜ì´ì§€ ë¶„ì„, ì´ìŠˆ ê°ì§€, ìë™ ìˆ˜ì • ì œì•ˆ
"""
import logging
import requests
from bs4 import BeautifulSoup
from typing import Dict, List, Optional
from urllib.parse import urlparse, urljoin
from datetime import datetime
import re

from .base import AnalyzerService

logger = logging.getLogger(__name__)


class SEOAdvisor(AnalyzerService):
    """
    AI-based SEO Advisor
    Analyzes pages, detects issues, and suggests improvements
    """

    # Severity levels
    SEVERITY_CRITICAL = 'critical'
    SEVERITY_WARNING = 'warning'
    SEVERITY_INFO = 'info'

    def __init__(self):
        super().__init__()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; SEOAnalyzerBot/1.0)'
        })
        self._last_fetch_error = None

    def analyze(self, page_url: str, pagespeed_data: Optional[Dict] = None, **kwargs) -> Dict:
        """
        Comprehensive SEO analysis of a page

        Args:
            page_url: URL to analyze
            pagespeed_data: PageSpeed Insights data (optional)
            **kwargs: Additional options

        Returns:
            {
                'url': 'https://...',
                'overall_health': 75,
                'issues': [...],
                'action_plan': {...},
                'auto_fix': {...},
                'estimated_time': 'ì•½ 2ì‹œê°„',
                'potential_score_gain': 15,
            }
        """
        self.log_info(f"ğŸ” Starting SEO analysis: {page_url}")
        self._last_fetch_error = None

        # 1. Fetch HTML
        html_content = self._fetch_html(page_url)
        if not html_content:
            error_msg = self._last_fetch_error or 'Failed to fetch page'
            return {'error': True, 'message': f'Failed to fetch page: {error_msg}'}

        soup = BeautifulSoup(html_content, 'html.parser')

        # 2. Analyze each category
        issues = []

        # Meta tags analysis
        issues.extend(self._analyze_meta_tags(soup, page_url))

        # Title tag analysis
        issues.extend(self._analyze_title_tag(soup))

        # Heading tags analysis
        issues.extend(self._analyze_headings(soup))

        # Image analysis
        issues.extend(self._analyze_images(soup, page_url))

        # Link analysis
        issues.extend(self._analyze_links(soup, page_url))

        # Content analysis
        issues.extend(self._analyze_content(soup))

        # PageSpeed data integration (if provided)
        if pagespeed_data:
            issues.extend(self._analyze_performance(pagespeed_data))

        # 3. Calculate health score
        overall_health = self._calculate_health_score(issues)

        # 4. Generate action plan
        action_plan = self._generate_action_plan(issues)

        # 5. Extract auto-fixable items
        auto_fix = self._extract_auto_fixable(issues)

        # 6. Calculate potential improvement
        potential_gain = self._calculate_potential_gain(issues)
        estimated_time = self._estimate_fix_time(issues)

        self.log_info(f"âœ… Analysis complete: {page_url} (health: {overall_health})")

        return {
            'url': page_url,
            'overall_health': overall_health,
            'issues': issues,
            'action_plan': action_plan,
            'auto_fix': auto_fix,
            'auto_fix_count': auto_fix.get('count', 0),
            'estimated_time': estimated_time['formatted'],
            'estimated_time_minutes': estimated_time['minutes'],
            'potential_score_gain': potential_gain,
            'timestamp': datetime.now().isoformat(),
            'error': False,
        }

    def _fetch_html(self, url: str) -> Optional[str]:
        """Fetch HTML from URL"""
        import requests
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            return response.text
        except requests.exceptions.HTTPError as e:
            # HTTP error (404, 500, etc.)
            status_code = getattr(e.response, 'status_code', None) if hasattr(e, 'response') else None
            if status_code:
                if status_code == 404:
                    self._last_fetch_error = "í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (HTTP 404)"
                else:
                    self._last_fetch_error = f"HTTP {status_code} ì—ëŸ¬"
                self.log_error(f"HTTP {status_code} error fetching {url}: {e}")
            else:
                self._last_fetch_error = "í˜ì´ì§€ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                self.log_error(f"HTTP error fetching {url}: {e}")
            return None
        except requests.exceptions.Timeout:
            self.log_error(f"Timeout fetching {url}")
            self._last_fetch_error = "ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (15ì´ˆ)"
            return None
        except Exception as e:
            self.log_error(f"Failed to fetch {url}: {e}")
            self._last_fetch_error = f"í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}"
            return None

    # ========== Analysis Methods ==========

    def _analyze_meta_tags(self, soup: BeautifulSoup, url: str) -> List[Dict]:
        """Analyze meta tags"""
        issues = []

        # Meta Description check
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if not meta_desc or not meta_desc.get('content'):
            issues.append({
                'type': 'meta_description_missing',
                'severity': self.SEVERITY_CRITICAL,
                'category': 'meta_tags',
                'title': 'ë©”íƒ€ ì„¤ëª… ëˆ„ë½',
                'message': 'ë©”íƒ€ ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ì— í‘œì‹œë  ì„¤ëª…ì„ ì¶”ê°€í•˜ì„¸ìš”.',
                'fix': '120-160ì ê¸¸ì´ì˜ ë§¤ë ¥ì ì¸ ì„¤ëª…ì„ ì‘ì„±í•˜ì„¸ìš”',
                'example': self._generate_meta_description_suggestion(soup, url),
                'auto_fix_available': True,
                'auto_fix_method': 'generate_meta_description',
                'impact': 'high',
            })
        elif meta_desc:
            content = meta_desc.get('content', '')
            if len(content) < 120:
                issues.append({
                    'type': 'meta_description_too_short',
                    'severity': self.SEVERITY_WARNING,
                    'category': 'meta_tags',
                    'title': 'ë©”íƒ€ ì„¤ëª…ì´ ë„ˆë¬´ ì§§ìŒ',
                    'message': f'í˜„ì¬ {len(content)}ìì…ë‹ˆë‹¤. 120-160ìê°€ ê¶Œì¥ë©ë‹ˆë‹¤.',
                    'current_value': content,
                    'fix': 'ë” ìì„¸í•œ ì„¤ëª…ìœ¼ë¡œ í™•ì¥í•˜ì„¸ìš”',
                    'auto_fix_available': True,
                    'auto_fix_method': 'expand_meta_description',
                    'impact': 'medium',
                })
            elif len(content) > 160:
                issues.append({
                    'type': 'meta_description_too_long',
                    'severity': self.SEVERITY_WARNING,
                    'category': 'meta_tags',
                    'title': 'ë©”íƒ€ ì„¤ëª…ì´ ë„ˆë¬´ ê¹€',
                    'message': f'í˜„ì¬ {len(content)}ìì…ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
                    'current_value': content,
                    'fix': '160ì ì´ë‚´ë¡œ ì¤„ì´ì„¸ìš”',
                    'suggested_value': content[:157] + '...',
                    'auto_fix_available': True,
                    'auto_fix_method': 'shorten_meta_description',
                    'impact': 'medium',
                })

        # Open Graph tags check
        og_title = soup.find('meta', property='og:title')
        og_desc = soup.find('meta', property='og:description')
        og_image = soup.find('meta', property='og:image')

        missing_og = []
        if not og_title:
            missing_og.append('og:title')
        if not og_desc:
            missing_og.append('og:description')
        if not og_image:
            missing_og.append('og:image')

        if missing_og:
            issues.append({
                'type': 'open_graph_incomplete',
                'severity': self.SEVERITY_WARNING,
                'category': 'meta_tags',
                'title': 'Open Graph íƒœê·¸ ë¶ˆì™„ì „',
                'message': 'ì†Œì…œ ë¯¸ë””ì–´ ê³µìœ  ìµœì í™”ë¥¼ ìœ„í•´ Open Graph íƒœê·¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.',
                'missing': missing_og,
                'fix': 'ëˆ„ë½ëœ Open Graph íƒœê·¸ ì¶”ê°€',
                'auto_fix_available': True,
                'auto_fix_method': 'generate_open_graph_tags',
                'impact': 'medium',
            })

        return issues

    def _analyze_title_tag(self, soup: BeautifulSoup) -> List[Dict]:
        """Analyze title tag"""
        issues = []

        title_tag = soup.find('title')
        if not title_tag or not title_tag.text.strip():
            issues.append({
                'type': 'title_missing',
                'severity': self.SEVERITY_CRITICAL,
                'category': 'title',
                'title': 'ì œëª© íƒœê·¸ ëˆ„ë½',
                'message': 'í˜ì´ì§€ ì œëª©ì´ ì—†ìŠµë‹ˆë‹¤. SEOì— ì¹˜ëª…ì ì…ë‹ˆë‹¤.',
                'fix': '50-60ì ê¸¸ì´ì˜ ëª…í™•í•œ ì œëª©ì„ ì‘ì„±í•˜ì„¸ìš”',
                'auto_fix_available': True,
                'auto_fix_method': 'generate_title',
                'impact': 'critical',
            })
        else:
            title_text = title_tag.text.strip()
            if len(title_text) < 30:
                issues.append({
                    'type': 'title_too_short',
                    'severity': self.SEVERITY_WARNING,
                    'category': 'title',
                    'title': 'ì œëª©ì´ ë„ˆë¬´ ì§§ìŒ',
                    'message': f'í˜„ì¬ {len(title_text)}ìì…ë‹ˆë‹¤. 50-60ìê°€ ê¶Œì¥ë©ë‹ˆë‹¤.',
                    'current_value': title_text,
                    'fix': 'ë” ìì„¸í•˜ê³  ì„¤ëª…ì ì¸ ì œëª©ìœ¼ë¡œ í™•ì¥í•˜ì„¸ìš”',
                    'auto_fix_available': True,
                    'auto_fix_method': 'expand_title',
                    'impact': 'medium',
                })
            elif len(title_text) > 60:
                issues.append({
                    'type': 'title_too_long',
                    'severity': self.SEVERITY_WARNING,
                    'category': 'title',
                    'title': 'ì œëª©ì´ ë„ˆë¬´ ê¹€',
                    'message': f'í˜„ì¬ {len(title_text)}ìì…ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
                    'current_value': title_text,
                    'fix': '60ì ì´ë‚´ë¡œ ì¤„ì´ì„¸ìš”',
                    'suggested_value': title_text[:57] + '...',
                    'auto_fix_available': True,
                    'auto_fix_method': 'shorten_title',
                    'impact': 'medium',
                })

        return issues

    def _analyze_headings(self, soup: BeautifulSoup) -> List[Dict]:
        """Analyze heading tags (H1-H6)"""
        issues = []

        # H1 tag check
        h1_tags = soup.find_all('h1')
        if not h1_tags:
            issues.append({
                'type': 'h1_missing',
                'severity': self.SEVERITY_CRITICAL,
                'category': 'headings',
                'title': 'H1 íƒœê·¸ ì—†ìŒ',
                'message': 'H1 íƒœê·¸ëŠ” í˜ì´ì§€ì˜ ì£¼ìš” ì œëª©ìœ¼ë¡œ í•„ìˆ˜ì…ë‹ˆë‹¤.',
                'fix': 'í˜ì´ì§€ ì£¼ì œë¥¼ ë‚˜íƒ€ë‚´ëŠ” H1 íƒœê·¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš”',
                'auto_fix_available': False,
                'impact': 'high',
            })
        elif len(h1_tags) > 1:
            issues.append({
                'type': 'multiple_h1',
                'severity': self.SEVERITY_WARNING,
                'category': 'headings',
                'title': 'ì—¬ëŸ¬ ê°œì˜ H1 íƒœê·¸',
                'message': f'{len(h1_tags)}ê°œì˜ H1 íƒœê·¸ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. í•˜ë‚˜ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.',
                'current_values': [h1.text.strip() for h1 in h1_tags],
                'fix': 'ê°€ì¥ ì¤‘ìš”í•œ ì œëª© í•˜ë‚˜ë§Œ H1ìœ¼ë¡œ ë‚¨ê¸°ê³ , ë‚˜ë¨¸ì§€ëŠ” H2ë¡œ ë³€ê²½í•˜ì„¸ìš”',
                'auto_fix_available': False,
                'impact': 'medium',
            })

        return issues

    def _analyze_images(self, soup: BeautifulSoup, page_url: str) -> List[Dict]:
        """Analyze images"""
        issues = []

        images = soup.find_all('img')
        images_without_alt = []

        for img in images:
            # Alt text check
            if not img.get('alt'):
                images_without_alt.append(img.get('src', 'unknown'))

        # Alt text missing issue
        if images_without_alt:
            issues.append({
                'type': 'images_without_alt',
                'severity': self.SEVERITY_WARNING,
                'category': 'images',
                'title': 'Alt í…ìŠ¤íŠ¸ ëˆ„ë½',
                'message': f'{len(images_without_alt)}ê°œ ì´ë¯¸ì§€ì— alt ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤.',
                'images': images_without_alt[:10],
                'fix': 'ëª¨ë“  ì´ë¯¸ì§€ì— ì„¤ëª…ì ì¸ alt í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš”',
                'auto_fix_available': True,
                'auto_fix_method': 'generate_alt_texts',
                'impact': 'medium',
            })

        return issues

    def _analyze_links(self, soup: BeautifulSoup, page_url: str) -> List[Dict]:
        """Analyze links"""
        issues = []

        all_links = soup.find_all('a', href=True)
        internal_links = []

        parsed_url = urlparse(page_url)
        base_domain = parsed_url.netloc

        for link in all_links:
            href = link['href']

            # Classify internal/external links
            if href.startswith('http'):
                link_domain = urlparse(href).netloc
                if base_domain in link_domain:
                    internal_links.append(href)
            elif href.startswith('/'):
                internal_links.append(href)

        # Internal link shortage warning
        if len(internal_links) < 3:
            issues.append({
                'type': 'low_internal_links',
                'severity': self.SEVERITY_INFO,
                'category': 'links',
                'title': 'ë‚´ë¶€ ë§í¬ ë¶€ì¡±',
                'message': f'ë‚´ë¶€ ë§í¬ê°€ {len(internal_links)}ê°œë¿ì…ë‹ˆë‹¤. 3-5ê°œ ê¶Œì¥.',
                'fix': 'ê´€ë ¨ í˜ì´ì§€ë¡œ ë§í¬ë¥¼ ì¶”ê°€í•˜ì—¬ ì‚¬ì´íŠ¸ êµ¬ì¡°ë¥¼ ê°•í™”í•˜ì„¸ìš”',
                'auto_fix_available': False,
                'impact': 'medium',
            })

        return issues

    def _analyze_content(self, soup: BeautifulSoup) -> List[Dict]:
        """Analyze content"""
        issues = []

        # Extract body text
        body_text = soup.get_text(separator=' ', strip=True)
        word_count = len(body_text.split())

        # Content length check
        if word_count < 300:
            issues.append({
                'type': 'thin_content',
                'severity': self.SEVERITY_WARNING,
                'category': 'content',
                'title': 'ì½˜í…ì¸  ë¶€ì¡±',
                'message': f'í˜„ì¬ {word_count}ë‹¨ì–´ì…ë‹ˆë‹¤. ìµœì†Œ 300ë‹¨ì–´ ê¶Œì¥.',
                'fix': 'ë” ìì„¸í•˜ê³  ìœ ìš©í•œ ì½˜í…ì¸ ë¥¼ ì¶”ê°€í•˜ì„¸ìš”',
                'auto_fix_available': False,
                'impact': 'high',
            })

        return issues

    def _analyze_performance(self, pagespeed_data: Dict) -> List[Dict]:
        """Analyze PageSpeed Insights data"""
        issues = []

        # LCP (Largest Contentful Paint) check
        lcp = pagespeed_data.get('lcp')
        if lcp and lcp > 2500:
            issues.append({
                'type': 'slow_lcp',
                'severity': self.SEVERITY_WARNING if lcp < 4000 else self.SEVERITY_CRITICAL,
                'category': 'performance',
                'title': 'LCP (ìµœëŒ€ ì½˜í…ì¸  ë Œë”ë§ ì‹œê°„) ëŠë¦¼',
                'message': f'í˜„ì¬ {lcp}msì…ë‹ˆë‹¤. 2500ms ì´í•˜ê°€ ê¶Œì¥ë©ë‹ˆë‹¤.',
                'current_value': lcp,
                'threshold': 2500,
                'fix': 'ì´ë¯¸ì§€ ìµœì í™”, ì„œë²„ ì‘ë‹µ ì‹œê°„ ê°œì„ , ë Œë”ë§ ì°¨ë‹¨ ë¦¬ì†ŒìŠ¤ ì œê±°',
                'auto_fix_available': False,
                'impact': 'critical',
            })

        # CLS (Cumulative Layout Shift) check
        cls = pagespeed_data.get('cls')
        if cls and cls > 0.1:
            issues.append({
                'type': 'high_cls',
                'severity': self.SEVERITY_WARNING if cls < 0.25 else self.SEVERITY_CRITICAL,
                'category': 'performance',
                'title': 'CLS (ëˆ„ì  ë ˆì´ì•„ì›ƒ ì´ë™) ë†’ìŒ',
                'message': f'í˜„ì¬ {cls}ì…ë‹ˆë‹¤. 0.1 ì´í•˜ê°€ ê¶Œì¥ë©ë‹ˆë‹¤.',
                'current_value': cls,
                'threshold': 0.1,
                'fix': 'ì´ë¯¸ì§€/ë™ì˜ìƒì— í¬ê¸° ì§€ì •, ë™ì  ì½˜í…ì¸  ìœ„ì¹˜ ê³ ì •',
                'auto_fix_available': False,
                'impact': 'high',
            })

        return issues

    # ========== Helper Methods ==========

    def _calculate_health_score(self, issues: List[Dict]) -> int:
        """Calculate health score (0-100)"""
        if not issues:
            return 100

        # Weight-based score calculation
        penalty = 0
        for issue in issues:
            if issue['severity'] == self.SEVERITY_CRITICAL:
                penalty += 15
            elif issue['severity'] == self.SEVERITY_WARNING:
                penalty += 7
            elif issue['severity'] == self.SEVERITY_INFO:
                penalty += 3

        score = max(0, 100 - penalty)
        return score

    def _generate_action_plan(self, issues: List[Dict]) -> Dict:
        """Generate action plan by priority"""
        critical = [i for i in issues if i['severity'] == self.SEVERITY_CRITICAL]
        warnings = [i for i in issues if i['severity'] == self.SEVERITY_WARNING]
        info = [i for i in issues if i['severity'] == self.SEVERITY_INFO]

        return {
            'immediate': critical,
            'this_week': warnings,
            'nice_to_have': info,
        }

    def _extract_auto_fixable(self, issues: List[Dict]) -> Dict:
        """Extract auto-fixable items"""
        auto_fixable = [i for i in issues if i.get('auto_fix_available')]

        return {
            'count': len(auto_fixable),
            'issues': auto_fixable,
            'methods': [i.get('auto_fix_method') for i in auto_fixable],
        }

    def _calculate_potential_gain(self, issues: List[Dict]) -> int:
        """Calculate expected score improvement"""
        gain = 0
        for issue in issues:
            if issue.get('auto_fix_available'):
                if issue['severity'] == self.SEVERITY_CRITICAL:
                    gain += 15
                elif issue['severity'] == self.SEVERITY_WARNING:
                    gain += 7
                elif issue['severity'] == self.SEVERITY_INFO:
                    gain += 3

        return min(gain, 35)

    def _estimate_fix_time(self, issues: List[Dict]) -> Dict:
        """Estimate fix time"""
        total_minutes = 0
        for issue in issues:
            if issue['severity'] == self.SEVERITY_CRITICAL:
                total_minutes += 20
            elif issue['severity'] == self.SEVERITY_WARNING:
                total_minutes += 10
            else:
                total_minutes += 5

        hours = total_minutes // 60
        minutes = total_minutes % 60

        if hours > 0:
            formatted = f"ì•½ {hours}ì‹œê°„ {minutes}ë¶„"
        else:
            formatted = f"ì•½ {minutes}ë¶„"

        return {
            'formatted': formatted,
            'minutes': total_minutes
        }

    def _generate_meta_description_suggestion(self, soup: BeautifulSoup, url: str) -> str:
        """Generate AI-based meta description (simple version)"""
        # Combine title and first sentence to generate meta description
        title = soup.find('title')
        title_text = title.text.strip() if title else ""

        # Extract first sentence from body
        paragraphs = soup.find_all('p')
        first_sentence = ""
        for p in paragraphs:
            text = p.get_text(strip=True)
            if len(text) > 50:
                first_sentence = text[:120]
                break

        if title_text and first_sentence:
            return f"{title_text}. {first_sentence}..."
        elif title_text:
            return f"{title_text}ì— ëŒ€í•œ ì™„ë²½í•œ ê°€ì´ë“œ. ì´ˆë³´ìë„ ì‰½ê²Œ ë”°ë¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        else:
            return "ì´ í˜ì´ì§€ì— ëŒ€í•œ ìì„¸í•œ ì •ë³´ì™€ ê°€ì´ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”."
