"""
Domain Refresh Service
ë„ë©”ì¸ ê°±ì‹  ë¡œì§ì„ ìº¡ìŠí™”í•œ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
"""
import logging
import time
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from django.db import transaction
from ..models import Domain, Page, SEOMetrics
from .domain_scanner import DomainScanner
from .pagespeed_insights import PageSpeedInsightsService
from .search_console import SearchConsoleService
from .rate_limiter import RateLimiter

logger = logging.getLogger(__name__)


class DomainRefreshService:
    """
    ë„ë©”ì¸ ë°ì´í„° ê°±ì‹  ì„œë¹„ìŠ¤

    ì´ ì„œë¹„ìŠ¤ëŠ” views.pyì˜ refresh()ì™€ tasks.pyì˜ refresh_domain_cache()ì—ì„œ
    ì‚¬ìš©ë˜ëŠ” ê³µí†µ ë¡œì§ì„ ìº¡ìŠí™”í•©ë‹ˆë‹¤.

    Usage:
        # ë™ê¸° ë°©ì‹ (views.py)
        service = DomainRefreshService(max_pages=100, max_metrics=5)
        result = service.refresh_domain(domain)

        # ë¹„ë™ê¸° ë°©ì‹ (tasks.py)
        service = DomainRefreshService(max_pages=1000, max_metrics=1000)
        result = service.refresh_domain(domain, progress_callback=callback)
    """

    def __init__(self, max_pages=100, max_metrics=None, mobile_only=True):
        """
        Initialize service

        Args:
            max_pages: ìµœëŒ€ í˜ì´ì§€ ë°œê²¬ ìˆ˜
            max_metrics: ë©”íŠ¸ë¦­ì„ ê°€ì ¸ì˜¬ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (None = ëª¨ë“  í˜ì´ì§€)
            mobile_only: Trueë©´ mobileë§Œ ë¶„ì„ (2ë°° ë¹ ë¦„), Falseë©´ mobile+desktop
        """
        self.max_pages = max_pages
        self.max_metrics = max_metrics or max_pages
        self.mobile_only = mobile_only

        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.scanner = DomainScanner(max_pages=max_pages)
        self.pagespeed = PageSpeedInsightsService()
        self.search_console = None
        self.search_console_available = False

        # Rate limiting for PageSpeed API
        # PageSpeed Insights API limits:
        # - 400 requests per 100 seconds = 4 requests per second
        # - 25,000 requests per day
        #
        # Strategy: Use thread-safe rate limiter
        # - Max 4 concurrent requests (Semaphore)
        # - Min 250ms interval between requests (Token bucket)
        self.rate_limiter = RateLimiter(
            max_requests_per_second=4.0,
            max_concurrent=4
        )

        # Search Console ì´ˆê¸°í™” ì‹œë„
        try:
            self.search_console = SearchConsoleService()
            self.search_console_available = True
            logger.info("Search Console service initialized successfully")
        except Exception as e:
            logger.warning(f"Search Console not available: {e}")

    def refresh_domain(self, domain, progress_callback=None):
        """
        ë„ë©”ì¸ ë°ì´í„° ê°±ì‹  ë©”ì¸ ë¡œì§

        Process:
        1. Discover pages from sitemap/crawling (0-10%)
        2. Create/update pages in database (10-70%)
        3. Build page hierarchy (70%)
        4. Fetch SEO metrics in parallel (70-90%)
        5. Update domain aggregates (90-100%)

        Args:
            domain: Domain ì¸ìŠ¤í„´ìŠ¤
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (optional)
                              callback(current, total, status_message)

        Returns:
            dict: {
                'pages_discovered': int,
                'pages_processed': int,
                'metrics_fetched': int,
                'domain': Domain
            }
        """
        start_time = time.time()
        analysis_mode = "mobile-only" if self.mobile_only else "mobile+desktop"

        logger.info(
            f"=== Starting domain refresh ===\n"
            f"  Domain: {domain.domain_name}\n"
            f"  Max pages: {self.max_pages}\n"
            f"  Max metrics: {self.max_metrics}\n"
            f"  Analysis mode: {analysis_mode}"
        )

        # Step 1: í˜ì´ì§€ ë°œê²¬ (0-10%)
        self._update_progress(progress_callback, 5, 100, "Discovering pages from sitemap")
        discovery_result = self.scanner.discover_from_domain(
            domain.domain_name,
            domain.protocol
        )

        discovered_pages = discovery_result['pages']
        total_pages = len(discovered_pages)
        logger.info(
            f"Discovery completed: {total_pages} pages found "
            f"({discovery_result.get('subdomains', [])!r} subdomains)"
        )

        # Step 2: í˜ì´ì§€ ì²˜ë¦¬ (10-70%)
        processed_pages = []
        metrics_fetched = 0

        self._update_progress(progress_callback, 10, 100, "Saving pages to database")

        # Step 2a: Create/update pages in single transaction (10-60%)
        with transaction.atomic():
            for idx, page_data in enumerate(discovered_pages):
                progress = 10 + int((idx + 1) / total_pages * 50)
                self._update_progress(
                    progress_callback,
                    progress,
                    100,
                    f"Saving page {idx + 1}/{total_pages}"
                )

                page = self._create_or_update_page(domain, page_data)
                processed_pages.append(page)

        logger.info(f"Saved {len(processed_pages)} pages to database")

        # Step 2b: Establish parent-child relationships in single transaction (60-70%)
        self._update_progress(progress_callback, 65, 100, "Building page hierarchy")
        with transaction.atomic():
            self._establish_parent_relationships(domain)
        logger.info("Page hierarchy established")

        # Step 2c: Fetch metrics (70-90%) - OUTSIDE transaction for parallel processing
        self._update_progress(progress_callback, 70, 100, f"Fetching SEO metrics ({analysis_mode})")
        metrics_to_fetch = processed_pages[:self.max_metrics]
        logger.info(f"Will fetch metrics for {len(metrics_to_fetch)} pages (limit: {self.max_metrics})")

        metrics_fetched = self._fetch_metrics_parallel(
            metrics_to_fetch,
            progress_callback
        )

        # Step 3: Update domain aggregates in single transaction (90-100%)
        self._update_progress(progress_callback, 92, 100, "Updating domain statistics")
        with transaction.atomic():
            domain.update_aggregate_scores()
            domain.last_scanned_at = datetime.now(timezone.utc)
            domain.save()
        logger.info("Domain aggregate scores updated")

        self._update_progress(progress_callback, 100, 100, "Scan completed successfully")

        # Summary
        elapsed = time.time() - start_time
        result = {
            'pages_discovered': total_pages,
            'pages_processed': len(processed_pages),
            'metrics_fetched': metrics_fetched,
            'domain': domain,
            'elapsed_time': elapsed
        }

        logger.info(
            f"=== Domain refresh completed ===\n"
            f"  Pages discovered: {total_pages}\n"
            f"  Pages processed: {len(processed_pages)}\n"
            f"  Metrics fetched: {metrics_fetched}\n"
            f"  Time elapsed: {elapsed:.1f}s\n"
            f"  Average: {metrics_fetched/elapsed:.2f} pages/sec"
        )

        return result

    def refresh_search_console_only(self, domain, progress_callback=None):
        """
        Search Console ë°ì´í„°ë§Œ ê°±ì‹  (PageSpeed ìŠ¤ìº” ì—†ì´)

        ì„¤ê³„ ì˜ë„:
        - Full Scanì€ ëŠë¦¬ê³  ë¹„ìš©ì´ ë†’ìŒ (PageSpeed API ì¿¼í„° ì†Œëª¨)
        - ìƒ‰ì¸ ìƒíƒœëŠ” ìì£¼ í™•ì¸í•´ì•¼ í•˜ì§€ë§Œ PageSpeedëŠ” ì£¼ê¸°ì ìœ¼ë¡œë§Œ í•„ìš”
        - ì´ ë©”ì„œë“œëŠ” ê¸°ì¡´ í˜ì´ì§€ì˜ Search Console ë°ì´í„°ë§Œ ì—…ë°ì´íŠ¸

        Process:
        1. ê¸°ì¡´ í˜ì´ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ìƒˆë¡œìš´ í˜ì´ì§€ ë°œê²¬ ì•ˆ í•¨)
        2. ê° í˜ì´ì§€ì˜ Search Console ë°ì´í„° ì—…ë°ì´íŠ¸
           - URL Inspection API â†’ ìƒ‰ì¸ ìƒíƒœ
           - Search Analytics API â†’ ë…¸ì¶œìˆ˜/í´ë¦­ìˆ˜
        3. ë„ë©”ì¸ í†µê³„ ì—…ë°ì´íŠ¸

        Advantages:
        - ë¹ ë¦„ (PageSpeed API í˜¸ì¶œ ì—†ìŒ)
        - API ì¿¼í„° ì ˆì•½
        - ìƒ‰ì¸ ìƒíƒœ ì¼ì¼ ì²´í¬ì— ì í•©

        Args:
            domain: Domain ì¸ìŠ¤í„´ìŠ¤
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (optional)

        Returns:
            dict: {
                'pages_updated': int,
                'pages_failed': int,
                'domain': Domain
            }
        """
        if not self.search_console_available:
            logger.error("Search Console service not available")
            return {
                'error': True,
                'message': 'Search Console service not initialized',
                'pages_updated': 0,
                'pages_failed': 0,
            }

        start_time = time.time()
        logger.info(
            f"=== Starting Search Console refresh (lightweight) ===\n"
            f"  Domain: {domain.domain_name}"
        )

        # Get existing pages with metrics
        pages = Page.objects.filter(domain=domain).prefetch_related('seo_metrics')
        total_pages = pages.count()

        if total_pages == 0:
            logger.warning(f"No pages found for domain {domain.domain_name}")
            return {
                'pages_updated': 0,
                'pages_failed': 0,
                'domain': domain,
            }

        logger.info(f"Found {total_pages} pages to update")

        # Update Search Console data using BATCH request (12x faster!)
        updated = 0
        failed = 0

        # Filter pages that have metrics
        valid_pages = [p for p in pages if p.seo_metrics.first() is not None]
        invalid_count = total_pages - len(valid_pages)
        if invalid_count > 0:
            logger.warning(f"{invalid_count} pages skipped (no metrics)")
            failed += invalid_count

        if valid_pages:
            # Progress: starting batch
            self._update_progress(
                progress_callback,
                10,
                100,
                f"Batch fetching index status for {len(valid_pages)} pages..."
            )

            # Prepare batch request
            site_url = f"sc-domain:{domain.domain_name}"
            page_urls = [p.url for p in valid_pages]

            # Execute batch URL Inspection (single HTTP request!)
            try:
                batch_results = self.search_console.batch_get_index_status(site_url, page_urls)

                # Progress: processing results
                self._update_progress(
                    progress_callback,
                    50,
                    100,
                    f"Processing {len(batch_results)} index status results..."
                )

                # Update each page with batch results
                for idx, (page, result) in enumerate(zip(valid_pages, batch_results)):
                    try:
                        latest_metrics = page.seo_metrics.first()
                        if not latest_metrics:
                            continue

                        if result.get('error'):
                            logger.warning(f"âš ï¸ Batch failed for {page.url}: {result.get('message')}")
                            failed += 1
                            continue

                        # Update index status from batch result
                        latest_metrics.is_indexed = result.get('is_indexed', False)
                        latest_metrics.index_status = result.get('verdict', 'UNKNOWN')
                        latest_metrics.coverage_state = result.get('coverage_state', 'Unknown')

                        # Fetch Search Analytics (not batchable in same way)
                        try:
                            analytics = self.search_console.get_page_analytics(
                                site_url,
                                page.url
                            )
                            if not analytics.get('error'):
                                latest_metrics.impressions = analytics.get('impressions', 0)
                                latest_metrics.clicks = analytics.get('clicks', 0)
                                latest_metrics.ctr = analytics.get('ctr', 0)
                                latest_metrics.avg_position = analytics.get('avg_position', 0)
                        except BaseException as analytics_error:
                            # Search Analytics failure is non-fatal
                            logger.warning(f"âš ï¸ Search Analytics failed for {page.url}: {analytics_error}")

                        latest_metrics.save()
                        updated += 1
                        logger.debug(f"âœ… Updated {page.url}")

                        # Progress update
                        progress = 50 + int((idx / len(valid_pages)) * 40)
                        self._update_progress(
                            progress_callback,
                            progress,
                            100,
                            f"Updated {idx+1}/{len(valid_pages)} pages"
                        )

                    except BaseException as e:
                        logger.error(f"âŒ Failed to update {page.url}: {e}", exc_info=True)
                        failed += 1

            except BaseException as batch_error:
                # Batch failed completely - fall back to sequential
                logger.error(f"âŒ Batch request failed, falling back to sequential: {batch_error}", exc_info=True)

                for idx, page in enumerate(valid_pages, 1):
                    try:
                        self._fetch_search_console_data(page)
                        updated += 1

                        # Progress update
                        progress = 10 + int((idx / len(valid_pages)) * 80)
                        self._update_progress(
                            progress_callback,
                            progress,
                            100,
                            f"Updating {idx}/{len(valid_pages)} (fallback)"
                        )
                    except BaseException as e:
                        logger.error(f"âŒ Failed to update {page.url}: {e}")
                        failed += 1

        # Update domain aggregates
        self._update_progress(progress_callback, 95, 100, "Updating domain statistics")
        with transaction.atomic():
            domain.update_aggregate_scores()
            domain.last_scanned_at = datetime.now(timezone.utc)
            domain.save()

        self._update_progress(progress_callback, 100, 100, "Search Console refresh completed")

        # Summary
        elapsed = time.time() - start_time
        result = {
            'pages_updated': updated,
            'pages_failed': failed,
            'domain': domain,
            'elapsed_time': elapsed,
        }

        logger.info(
            f"=== Search Console refresh completed ===\n"
            f"  Pages updated: {updated}\n"
            f"  Pages failed: {failed}\n"
            f"  Time elapsed: {elapsed:.1f}s"
        )

        return result

    def _create_or_update_page(self, domain, page_data):
        """
        í˜ì´ì§€ ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸ (ìˆ˜ë™ í¸ì§‘ ë³´ì¡´)

        ìˆ˜ë™ í¸ì§‘ ë³´ì¡´ ê·œì¹™:
        1. use_manual_position=Trueë©´ depth_level ì—…ë°ì´íŠ¸ ì•ˆ í•¨
        2. last_manually_edited_atì´ ìˆìœ¼ë©´ parent_page ì—…ë°ì´íŠ¸ ì•ˆ í•¨
        3. ë©”íƒ€ë°ì´í„°(title, description)ëŠ” í•­ìƒ ì—…ë°ì´íŠ¸
        4. is_subdomain, subdomainì€ ìë™ ìŠ¤ìº” ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸

        Args:
            domain: Domain ì¸ìŠ¤í„´ìŠ¤
            page_data: í˜ì´ì§€ ì •ë³´ dict

        Returns:
            Page ì¸ìŠ¤í„´ìŠ¤
        """
        page, created = Page.objects.get_or_create(
            domain=domain,
            url=page_data['url'],
            defaults={
                'path': page_data['path'],
                'is_subdomain': page_data['is_subdomain'],
                'subdomain': page_data['subdomain'],
                'depth_level': page_data.get('depth_level', 0),
                'status': 'active',
            }
        )

        if not created:
            # ê¸°ì¡´ í˜ì´ì§€ - ìˆ˜ë™ í¸ì§‘ ë³´ì¡´
            update_fields = []

            # PathëŠ” í•­ìƒ ì—…ë°ì´íŠ¸ (URL êµ¬ì¡° ë³€ê²½ ë°˜ì˜)
            if page.path != page_data['path']:
                page.path = page_data['path']
                update_fields.append('path')

            # is_subdomain, subdomainì€ ìë™ ìŠ¤ìº” ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
            if page.is_subdomain != page_data['is_subdomain']:
                page.is_subdomain = page_data['is_subdomain']
                update_fields.append('is_subdomain')

            if page.subdomain != page_data.get('subdomain'):
                page.subdomain = page_data.get('subdomain')
                update_fields.append('subdomain')

            # depth_levelì€ ìˆ˜ë™ ìœ„ì¹˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ ì—…ë°ì´íŠ¸
            if not page.use_manual_position:
                if page.depth_level != page_data.get('depth_level', 0):
                    page.depth_level = page_data.get('depth_level', 0)
                    update_fields.append('depth_level')

            # StatusëŠ” í•­ìƒ activeë¡œ (í˜ì´ì§€ê°€ ë°œê²¬ë˜ì—ˆìœ¼ë¯€ë¡œ)
            if page.status != 'active':
                page.status = 'active'
                update_fields.append('status')

            # ë©”íƒ€ë°ì´í„°ëŠ” í•­ìƒ ì—…ë°ì´íŠ¸ (page_dataì— ìˆëŠ” ê²½ìš°)
            if 'title' in page_data and page.title != page_data['title']:
                page.title = page_data['title']
                update_fields.append('title')

            if 'description' in page_data and page.description != page_data['description']:
                page.description = page_data['description']
                update_fields.append('description')

            # ë³€ê²½ì‚¬í•­ì´ ìˆìœ¼ë©´ ì €ì¥
            if update_fields:
                page.save(update_fields=update_fields)
                logger.debug(f"Updated page {page.url}: {update_fields}")
            else:
                logger.debug(f"No changes for page: {page.url}")
        else:
            logger.debug(f"Created new page: {page.url}")

        return page

    def _establish_parent_relationships(self, domain):
        """
        í˜ì´ì§€ ê°„ ë¶€ëª¨-ìì‹ ê´€ê³„ ì„¤ì • ë° depth_level ì¬ê³„ì‚°

        ìˆ˜ë™ í¸ì§‘ ë³´ì¡´ ê·œì¹™:
        - last_manually_edited_atì´ ìˆëŠ” í˜ì´ì§€ëŠ” parent_page ì—…ë°ì´íŠ¸ ì•ˆ í•¨
        - use_manual_position=Trueì¸ í˜ì´ì§€ëŠ” depth_level ì—…ë°ì´íŠ¸ ì•ˆ í•¨

        Args:
            domain: Domain ì¸ìŠ¤í„´ìŠ¤
        """
        # Get all pages for this domain
        pages = list(Page.objects.filter(domain=domain).order_by('path'))

        # Separate manually edited and auto pages
        manually_edited_pages = [p for p in pages if p.last_manually_edited_at]
        auto_pages = [p for p in pages if not p.last_manually_edited_at]

        logger.info(
            f"Establishing parent relationships: "
            f"{len(auto_pages)} auto pages, {len(manually_edited_pages)} manually edited (skipped)"
        )

        # First, find the root page (shortest path, usually '/')
        root_page = None
        for page in pages:
            if page.path == '/' or page.path == '':
                root_page = page
                # Only update if not manually edited
                if not page.last_manually_edited_at:
                    page.depth_level = 0
                    page.parent_page = None
                    page.save(update_fields=['parent_page', 'depth_level'])
                    logger.debug(f"Set root page: {page.url} (depth 0)")
                else:
                    logger.debug(f"Skipped manually edited root page: {page.url}")
                break

        # If no explicit root found, use the page with the shortest path
        if not root_page and pages:
            root_page = min(pages, key=lambda p: len(p.path.strip('/')))
            # Only update if not manually edited
            if not root_page.last_manually_edited_at:
                root_page.depth_level = 0
                root_page.parent_page = None
                root_page.save(update_fields=['parent_page', 'depth_level'])
                logger.debug(f"Set root page (shortest path): {root_page.url} (depth 0)")
            else:
                logger.debug(f"Skipped manually edited root (shortest path): {root_page.url}")

        # Sort pages by path length (shallow to deep)
        pages_sorted = sorted(pages, key=lambda p: len(p.path.strip('/')))

        # Build parent-child relationships and calculate depth
        # Only for pages that are NOT manually edited
        for page in pages_sorted:
            # Skip manually edited pages
            if page.last_manually_edited_at:
                logger.debug(f"Skipping manually edited page: {page.url}")
                continue

            if page == root_page:
                continue

            page_path = page.path.strip('/')

            # Find the best parent (longest matching path prefix)
            best_parent = None
            longest_match = -1

            for potential_parent in pages_sorted:
                if potential_parent == page:
                    continue

                parent_path = potential_parent.path.strip('/')

                # Check if this page is under the potential parent's path
                if not parent_path and potential_parent == root_page:
                    # Root page can be a parent
                    if longest_match < 0:
                        best_parent = potential_parent
                        longest_match = 0
                elif parent_path and page_path.startswith(parent_path + '/'):
                    # This is a valid parent with matching path
                    if len(parent_path) > longest_match:
                        best_parent = potential_parent
                        longest_match = len(parent_path)

            # Set parent and calculate depth
            update_fields = []
            if best_parent:
                page.parent_page = best_parent
                update_fields.append('parent_page')
                # Only update depth if not using manual position
                if not page.use_manual_position:
                    page.depth_level = best_parent.depth_level + 1
                    update_fields.append('depth_level')
            else:
                # No parent found, make it a child of root
                page.parent_page = root_page
                update_fields.append('parent_page')
                # Only update depth if not using manual position
                if not page.use_manual_position:
                    page.depth_level = 1
                    update_fields.append('depth_level')

            if update_fields:
                page.save(update_fields=update_fields)
                logger.debug(
                    f"Set parent: {page.path} (depth {page.depth_level}) -> "
                    f"{page.parent_page.path if page.parent_page else 'None'}"
                )

    def _fetch_metrics_parallel(self, pages, progress_callback):
        """
        ë³‘ë ¬ë¡œ í˜ì´ì§€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (ìµœì í™”ëœ Rate limiting)

        ìµœì í™” ì „ëµ:
        1. Batch URL Inspection (8-12x faster) - ë‹¨ì¼ HTTP ìš”ì²­ìœ¼ë¡œ ëª¨ë“  í˜ì´ì§€ ìƒ‰ì¸ ìƒíƒœ í™•ì¸
        2. Thread pool (10 workers) - ë†’ì€ ì²˜ë¦¬ëŸ‰
        3. Rate limiter (4 concurrent, 4 req/sec) - API ì œí•œ ì¤€ìˆ˜
        4. As-completed pattern - íš¨ìœ¨ì ì¸ ê²°ê³¼ ì²˜ë¦¬
        5. Mobile + Desktop ë³‘ë ¬ ë¶„ì„ - í¬ê´„ì ì¸ ë°ì´í„°

        Args:
            pages: Page ì¸ìŠ¤í„´ìŠ¤ ë¦¬ìŠ¤íŠ¸
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜

        Returns:
            int: ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­ ê°œìˆ˜
        """
        if not pages:
            return 0

        total_pages = len(pages)
        metrics_fetched = 0
        failed_pages = 0
        start_time = time.time()

        # Worker configuration
        # More workers than rate limit allows better CPU utilization
        # while rate limiter controls actual API calls
        max_workers = 10

        analysis_mode = "mobile-only" if self.mobile_only else "mobile+desktop"
        logger.info(
            f"Starting parallel metrics fetch:\n"
            f"  - Pages: {total_pages}\n"
            f"  - Workers: {max_workers}\n"
            f"  - Analysis mode: {analysis_mode}\n"
            f"  - Rate limit: 4 concurrent, 4 req/sec"
        )

        # OPTIMIZATION: Batch fetch index status for all pages at once (8-12x faster!)
        # This reduces sequential URL Inspection API calls from 17Ã—15s=255s to 1Ã—20s=20s
        self.index_status_cache = {}  # Cache for batch results
        if self.search_console_available and pages:
            try:
                site_url = f"sc-domain:{pages[0].domain.domain_name}"
                page_urls = [p.url for p in pages]

                logger.info(f"ğŸš€ Batch fetching index status for {len(page_urls)} pages...")
                self._update_progress(
                    progress_callback,
                    70,
                    100,
                    f"Batch fetching index status for {len(page_urls)} pages..."
                )

                # Execute batch URL Inspection (single HTTP request!)
                batch_results = self.search_console.batch_get_index_status(site_url, page_urls)

                # Cache results by URL for quick lookup during parallel processing
                for result in batch_results:
                    page_url = result.get('page_url')
                    if page_url:
                        self.index_status_cache[page_url] = result

                success_count = len([r for r in batch_results if not r.get('error')])
                logger.info(f"âœ… Batch index status complete: {success_count}/{len(page_urls)} successful")

            except BaseException as batch_error:
                # Batch failed - will fall back to sequential in _fetch_search_console_data
                logger.error(f"âŒ Batch URL Inspection failed, will use sequential fallback: {batch_error}", exc_info=True)
                self.index_status_cache = {}

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all tasks immediately
            # Rate limiting happens inside _fetch_page_metrics
            future_to_page = {
                executor.submit(self._fetch_page_metrics, page): page
                for page in pages
            }

            logger.info(f"Submitted all {total_pages} tasks to thread pool")

            # Process completed tasks as they finish
            for future in as_completed(future_to_page):
                page = future_to_page[future]

                try:
                    success = future.result()
                    if success:
                        metrics_fetched += 1
                    else:
                        failed_pages += 1

                except Exception as e:
                    failed_pages += 1
                    logger.error(
                        f"Exception processing {page.url}: {e}",
                        exc_info=True
                    )

                # Update progress (70-90% of total progress)
                progress_percent = 70 + int((metrics_fetched + failed_pages) / total_pages * 20)
                self._update_progress(
                    progress_callback,
                    progress_percent,
                    100,
                    f"Metrics: {metrics_fetched} succeeded, {failed_pages} failed ({analysis_mode})"
                )

        # Summary
        elapsed = time.time() - start_time
        success_rate = (metrics_fetched / total_pages * 100) if total_pages > 0 else 0
        pages_per_sec = metrics_fetched / elapsed if elapsed > 0 else 0

        logger.info(
            f"Parallel metrics fetch completed:\n"
            f"  - Succeeded: {metrics_fetched}/{total_pages} ({success_rate:.1f}%)\n"
            f"  - Failed: {failed_pages}/{total_pages}\n"
            f"  - Time: {elapsed:.1f}s\n"
            f"  - Rate: {pages_per_sec:.2f} pages/sec"
        )

        return metrics_fetched

    def _fetch_page_metrics(self, page):
        """
        í˜ì´ì§€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (Thread-safe rate limiting)

        Args:
            page: Page ì¸ìŠ¤í„´ìŠ¤

        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            # Apply rate limiting using context manager
            with self.rate_limiter:
                logger.info(f"Fetching metrics for {page.url}")

                # PageSpeed Insights ë¶„ì„ (mobile + desktop)
                metrics_result = self.pagespeed.analyze_both_strategies(
                    page.url,
                    mobile_only=self.mobile_only
                )

                if metrics_result.get('error'):
                    error_msg = metrics_result.get('message', 'Unknown error')
                    logger.error(f"PageSpeed error for {page.url}: {error_msg}")
                    return False

            # Save metrics in separate transaction (outside rate limiter)
            # This prevents lock timeouts from parallel processing
            try:
                with transaction.atomic():
                    self._save_pagespeed_metrics(page, metrics_result)

                    # Search Console ë°ì´í„° (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
                    if self.search_console_available:
                        try:
                            self._fetch_search_console_data(page)
                        except BaseException as sc_error:
                            # Search Console ì—ëŸ¬ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ (BaseExceptionìœ¼ë¡œ ëª¨ë“  ì—ëŸ¬ ì¡ìŒ)
                            logger.warning(f"Search Console data fetch failed for {page.url}: {sc_error}", exc_info=True)

                logger.info(f"Successfully saved metrics for {page.url}")
                return True

            except Exception as db_error:
                logger.error(f"Database error saving metrics for {page.url}: {db_error}", exc_info=True)
                return False

        except Exception as e:
            logger.error(f"Failed to fetch metrics for {page.url}: {e}", exc_info=True)
            return False

    def _save_pagespeed_metrics(self, page, metrics_result):
        """
        PageSpeed Insights ë©”íŠ¸ë¦­ ì €ì¥

        Args:
            page: Page ì¸ìŠ¤í„´ìŠ¤
            metrics_result: PageSpeed ë¶„ì„ ê²°ê³¼
        """
        primary = metrics_result['primary_scores']
        mobile = metrics_result['mobile']

        SEOMetrics.objects.create(
            page=page,
            seo_score=primary.get('seo_score'),
            performance_score=primary.get('performance_score'),
            accessibility_score=primary.get('accessibility_score'),
            best_practices_score=primary.get('best_practices_score'),
            pwa_score=primary.get('pwa_score'),
            mobile_score=primary.get('mobile_score'),
            desktop_score=primary.get('desktop_score'),
            lcp=mobile.get('lcp'),
            fid=mobile.get('fid'),
            cls=mobile.get('cls'),
            fcp=mobile.get('fcp'),
            tti=mobile.get('tti'),
            tbt=mobile.get('tbt'),
            snapshot_date=datetime.now(timezone.utc),
        )

    def _fetch_search_console_data(self, page):
        """
        Search Console ë°ì´í„° ìˆ˜ì§‘ ë° ì—…ë°ì´íŠ¸

        Responsibility:
        - Fetch accurate index status via URL Inspection API
        - Fetch search analytics (impressions, clicks) via Search Analytics API
        - Update existing SEOMetrics with Search Console data
        - Handle API failures gracefully (non-fatal)

        Process:
        1. Get latest SEOMetrics for page (must already exist from PageSpeed scan)
        2. Call URL Inspection API â†’ Update is_indexed, index_status, coverage_state
        3. Call Search Analytics API â†’ Update impressions, clicks, CTR, avg_position

        Error Handling:
        - API failures are logged but don't stop the scan
        - Individual page failures don't affect other pages
        - Missing data results in fields staying as default values

        Args:
            page: Page instance to fetch data for

        Notes:
        - Requires Search Console service to be initialized
        - Uses sc-domain: format for site URL
        - Called after PageSpeed metrics are collected
        - Non-fatal: failures are acceptable and logged
        """
        # Worker í¬ë˜ì‹œ ë°©ì§€: ëª¨ë“  ì—ëŸ¬ë¥¼ ì•ˆì „í•˜ê²Œ ì¡ìŒ
        try:
            # Search Console ê°ì²´ ìœ íš¨ì„± ê²€ì‚¬
            if not self.search_console or not hasattr(self.search_console, 'get_index_status'):
                logger.warning(f"Search Console not properly initialized for {page.url}")
                return

            # Site URL êµ¬ì„±
            site_url = f"sc-domain:{page.domain.domain_name}"

            # ìµœì‹  ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ ìƒì„±ëœ ìƒíƒœì—¬ì•¼ í•¨)
            latest_metrics = page.seo_metrics.first()
            if not latest_metrics:
                logger.warning(f"No SEO metrics found for {page.url}, skipping Search Console data")
                return

            # 1. URL Inspection APIë¡œ ì‹¤ì œ ìƒ‰ì¸ ìƒíƒœ í™•ì¸
            try:
                # Use cached batch result if available (8-12x faster!)
                index_status = None
                if hasattr(self, 'index_status_cache') and page.url in self.index_status_cache:
                    index_status = self.index_status_cache[page.url]
                    logger.debug(f"Using cached batch index status for {page.url}")
                else:
                    # Fallback to sequential API call if batch failed or not available
                    logger.debug(f"Fetching index status sequentially for {page.url}")
                    index_status = self.search_console.get_index_status(site_url, page.url)

                if index_status and not index_status.get('error'):
                    # SEOMetricsì— ìƒ‰ì¸ ìƒíƒœ ì €ì¥
                    latest_metrics.is_indexed = index_status.get('is_indexed', False)
                    latest_metrics.index_status = index_status.get('verdict', 'UNKNOWN')
                    latest_metrics.coverage_state = index_status.get('coverage_state', 'Unknown')
                    latest_metrics.save(update_fields=['is_indexed', 'index_status', 'coverage_state'])

                    logger.info(
                        f"Index status for {page.url}: "
                        f"is_indexed={latest_metrics.is_indexed}, verdict={latest_metrics.index_status}"
                    )
                else:
                    logger.warning(f"Failed to get index status for {page.url}: {index_status.get('message') if index_status else 'No result'}")

            except BaseException as index_error:
                # BaseExceptionìœ¼ë¡œ ëª¨ë“  ì—ëŸ¬ ì¡ìŒ (Google API ì €ë ˆë²¨ ì—ëŸ¬ í¬í•¨)
                logger.error(f"URL Inspection failed for {page.url}: {index_error}", exc_info=True)

            # 2. Search Analyticsë¡œ ë…¸ì¶œìˆ˜/í´ë¦­ìˆ˜ ê°€ì ¸ì˜¤ê¸° (sc-domain format)
            try:
                analytics = self.search_console.get_page_analytics(
                    site_url,  # Use sc-domain format
                    page.url
                )

                if not analytics.get('error'):
                    # SEOMetricsì— analytics ë°ì´í„° ì €ì¥
                    latest_metrics.impressions = analytics.get('impressions', 0)
                    latest_metrics.clicks = analytics.get('clicks', 0)
                    latest_metrics.ctr = analytics.get('ctr', 0)
                    latest_metrics.avg_position = analytics.get('avg_position', 0)
                    latest_metrics.save(update_fields=['impressions', 'clicks', 'ctr', 'avg_position'])

                    logger.debug(f"Updated Search Console analytics for {page.url}")
                else:
                    logger.warning(f"Failed to get analytics for {page.url}: {analytics.get('message')}")

            except BaseException as analytics_error:
                # BaseExceptionìœ¼ë¡œ ëª¨ë“  ì—ëŸ¬ ì¡ìŒ (ì—°ê²° ëŠê¹€ ë“±)
                logger.error(f"Search Analytics failed for {page.url}: {analytics_error}", exc_info=True)

        except BaseException as e:
            # ìµœìƒìœ„ ë³´í˜¸: Worker í¬ë˜ì‹œ ë°©ì§€
            logger.error(f"Critical error in Search Console data fetch for {page.url}: {e}", exc_info=True)

    @staticmethod
    def _update_progress(callback, current, total, message):
        """
        ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ

        Args:
            callback: ì½œë°± í•¨ìˆ˜
            current: í˜„ì¬ ì§„í–‰
            total: ì „ì²´
            message: ìƒíƒœ ë©”ì‹œì§€
        """
        logger.info(f"Progress update: {current}/{total} - {message}")
        logger.info(f"Callback is: {callback}, Type: {type(callback)}")
        if callback:
            try:
                logger.info(f"Calling callback with: current={current}, total={total}, message={message}")
                callback(current, total, message)
                logger.info(f"Progress callback executed successfully")
            except Exception as e:
                logger.error(f"Progress callback error: {e}", exc_info=True)
        else:
            logger.warning(f"No callback provided for progress update")
